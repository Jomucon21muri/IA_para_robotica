# ğŸ‘ï¸ VisiÃ³n por Computadora - Sistema Visual RobÃ³tico

## PropÃ³sito

Implementar el **sentido de la vista** del robot humanoide mediante cÃ¡maras y algoritmos de visiÃ³n artificial. Este mÃ³dulo es la base de la **Inteligencia Espacial-Visual** del robot, permitiÃ©ndole percibir, entender e interactuar con su entorno visual.

---

## ğŸŒŸ Capacidades del Sistema Visual

### Funciones Principales

| Capacidad | DescripciÃ³n | TecnologÃ­a | Carpeta |
|-----------|-------------|-----------|----------|
| **DetecciÃ³n de objetos** | Identificar y localizar objetos en la escena | YOLO, Faster R-CNN | `detection/` |
| **SegmentaciÃ³n** | Separar objetos por pÃ­xeles (semÃ¡ntica/instancia) | Mask R-CNN, U-Net | `segmentation/` |
| **Reconocimiento facial** | Identificar y verificar personas | FaceNet, ArcFace | `face_recognition/` |
| **EstimaciÃ³n de pose** | Detectar posiciones de personas y objetos | OpenPose, MediaPipe | `pose_estimation/` |
| **SLAM Visual** | Mapeo y localizaciÃ³n simultÃ¡neos | ORB-SLAM3, RTAB-Map | `slam/` |
| **PercepciÃ³n 3D** | ReconstrucciÃ³n tridimensional | EstÃ©reo, RGB-D | `depth/` |
| **Seguimiento de objetos** | Rastrear objetos en movimiento | SORT, DeepSORT | `tracking/` |
| **Reconocimiento de gestos** | Interpretar gestos humanos | MediaPipe, CNN | `gestures/` |

---

## ğŸ“‚ Estructura del Directorio

```
06_Vision/
â”œâ”€â”€ README.md (este archivo)
â”œâ”€â”€ detection/                      # DetecciÃ³n de objetos
â”‚   â”œâ”€â”€ yolo/                      # YOLO v5/v8
â”‚   â”œâ”€â”€ faster_rcnn/               # Faster R-CNN
â”‚   â”œâ”€â”€ real_time/                 # Optimizaciones para tiempo real
â”‚   â””â”€â”€ custom_datasets/           # Datasets personalizados
â”œâ”€â”€ segmentation/                   # SegmentaciÃ³n
â”‚   â”œâ”€â”€ semantic/                  # SegmentaciÃ³n semÃ¡ntica
â”‚   â”œâ”€â”€ instance/                  # SegmentaciÃ³n por instancia
â”‚   â””â”€â”€ panoptic/                  # SegmentaciÃ³n panÃ³ptica
â”œâ”€â”€ face_recognition/               # Reconocimiento facial
â”‚   â”œâ”€â”€ detection/                 # DetecciÃ³n de rostros
â”‚   â”œâ”€â”€ recognition/               # IdentificaciÃ³n
â”‚   â”œâ”€â”€ emotion/                   # DetecciÃ³n de emociones
â”‚   â””â”€â”€ face_database/             # Base de datos de rostros
â”œâ”€â”€ pose_estimation/                # EstimaciÃ³n de pose
â”‚   â”œâ”€â”€ human_pose/                # Pose humana 2D/3D
â”‚   â”œâ”€â”€ hand_tracking/             # Seguimiento de manos
â”‚   â””â”€â”€ body_tracking/             # Tracking corporal completo
â”œâ”€â”€ slam/                          # SLAM Visual
â”‚   â”œâ”€â”€ orb_slam/                  # ORB-SLAM3
â”‚   â”œâ”€â”€ rtabmap/                   # RTAB-Map
â”‚   â””â”€â”€ visual_odometry/           # OdometrÃ­a visual
â”œâ”€â”€ depth/                         # PercepciÃ³n de profundidad
â”‚   â”œâ”€â”€ stereo_vision/             # VisiÃ³n estÃ©reo
â”‚   â”œâ”€â”€ rgbd/                      # RGB-D (RealSense, Kinect)
â”‚   â””â”€â”€ depth_estimation/          # EstimaciÃ³n monocular
â”œâ”€â”€ tracking/                      # Seguimiento de objetos
â”‚   â”œâ”€â”€ sort/                      # SORT algorithm
â”‚   â”œâ”€â”€ deepsort/                  # DeepSORT
â”‚   â””â”€â”€ multi_object/              # Tracking multi-objeto
â”œâ”€â”€ gestures/                      # Reconocimiento de gestos
â”‚   â”œâ”€â”€ hand_gestures/             # Gestos de manos
â”‚   â”œâ”€â”€ body_gestures/             # Gestos corporales
â”‚   â””â”€â”€ gesture_commands/          # Comandos por gestos
â”œâ”€â”€ ocr/                           # Reconocimiento de texto
â”‚   â”œâ”€â”€ text_detection/            # DetecciÃ³n de texto
â”‚   â”œâ”€â”€ text_recognition/          # OCR (Tesseract, EasyOCR)
â”‚   â””â”€â”€ scene_text/                # Texto en escenas naturales
â”œâ”€â”€ preprocessing/                  # Preprocesamiento
â”‚   â”œâ”€â”€ calibration/               # CalibraciÃ³n de cÃ¡maras
â”‚   â”œâ”€â”€ enhancement/               # Mejora de imagen
â”‚   â””â”€â”€ augmentation/              # Data augmentation
â”œâ”€â”€ models/                        # Modelos entrenados
â”‚   â”œâ”€â”€ pretrained/                # Modelos preentrenados
â”‚   â””â”€â”€ finetuned/                 # Modelos ajustados
â”œâ”€â”€ datasets/                      # Datasets
â”‚   â”œâ”€â”€ coco/                      # COCO dataset
â”‚   â”œâ”€â”€ kitti/                     # KITTI (conducciÃ³n)
â”‚   â”œâ”€â”€ custom/                    # Datos propios
â”‚   â””â”€â”€ scripts/                   # Scripts de conversiÃ³n
â””â”€â”€ notebooks/                     # Notebooks de demostraciÃ³n
    â”œâ”€â”€ object_detection_demo.ipynb
    â”œâ”€â”€ face_recognition_demo.ipynb
    â”œâ”€â”€ slam_visualization.ipynb
    â””â”€â”€ depth_estimation_demo.ipynb
```

---

## 1. ğŸ‘ï¸ DetecciÃ³n de Objetos

### ImplementaciÃ³n con YOLO v8

```python
from ultralytics import YOLO
import cv2
import numpy as np

class ObjectDetector:
    def __init__(self, model_path='yolov8n.pt', conf_threshold=0.5):
        """
        Inicializa detector de objetos
        model_path: yolov8n (nano), yolov8s (small), yolov8m (medium), yolov8l (large)
        """
        self.model = YOLO(model_path)
        self.conf_threshold = conf_threshold
        self.class_names = self.model.names
        
    def detect(self, frame):
        """Detecta objetos en un frame"""
        results = self.model(frame, conf=self.conf_threshold, verbose=False)
        
        detections = []
        for r in results:
            boxes = r.boxes
            for box in boxes:
                # Extraer informaciÃ³n
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                conf = float(box.conf[0])
                cls = int(box.cls[0])
                class_name = self.class_names[cls]
                
                detections.append({
                    'bbox': [int(x1), int(y1), int(x2), int(y2)],
                    'confidence': conf,
                    'class': class_name,
                    'class_id': cls
                })
        
        return detections
    
    def draw_detections(self, frame, detections):
        """Dibuja las detecciones en el frame"""
        for det in detections:
            x1, y1, x2, y2 = det['bbox']
            conf = det['confidence']
            label = f"{det['class']} {conf:.2f}"
            
            # Dibujar bounding box
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # Dibujar etiqueta
            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)
            cv2.rectangle(frame, (x1, y1 - 20), (x1 + w, y1), (0, 255, 0), -1)
            cv2.putText(frame, label, (x1, y1 - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)
        
        return frame

# Uso en tiempo real
detector = ObjectDetector('yolov8n.pt')
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    detections = detector.detect(frame)
    frame = detector.draw_detections(frame, detections)
    
    cv2.imshow('Object Detection', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

### IntegraciÃ³n con ROS

```python
#!/usr/bin/env python3
import rospy
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose
from cv_bridge import CvBridge
import cv2

class YOLODetectorNode:
    def __init__(self):
        rospy.init_node('yolo_detector', anonymous=True)
        
        self.bridge = CvBridge()
        self.detector = ObjectDetector('yolov8n.pt')
        
        # Subscribers
        self.image_sub = rospy.Subscriber('/camera/image_raw', Image, self.image_callback)
        
        # Publishers
        self.detection_pub = rospy.Publisher('/vision/detections', Detection2DArray, queue_size=10)
        self.viz_pub = rospy.Publisher('/vision/detection_viz', Image, queue_size=10)
        
    def image_callback(self, msg):
        # Convertir ROS Image a OpenCV
        cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
        
        # Detectar objetos
        detections = self.detector.detect(cv_image)
        
        # Publicar detecciones
        detection_msg = Detection2DArray()
        detection_msg.header = msg.header
        
        for det in detections:
            detection_2d = Detection2D()
            detection_2d.bbox.center.x = (det['bbox'][0] + det['bbox'][2]) / 2
            detection_2d.bbox.center.y = (det['bbox'][1] + det['bbox'][3]) / 2
            detection_2d.bbox.size_x = det['bbox'][2] - det['bbox'][0]
            detection_2d.bbox.size_y = det['bbox'][3] - det['bbox'][1]
            
            hypothesis = ObjectHypothesisWithPose()
            hypothesis.id = det['class_id']
            hypothesis.score = det['confidence']
            detection_2d.results.append(hypothesis)
            
            detection_msg.detections.append(detection_2d)
        
        self.detection_pub.publish(detection_msg)
        
        # Publicar visualizaciÃ³n
        viz_image = self.detector.draw_detections(cv_image.copy(), detections)
        viz_msg = self.bridge.cv2_to_imgmsg(viz_image, "bgr8")
        self.viz_pub.publish(viz_msg)

if __name__ == '__main__':
    try:
        node = YOLODetectorNode()
        rospy.spin()
    except rospy.ROSInterruptException:
        pass
```

---

## 2. ğŸ­ Reconocimiento Facial y Emociones

### DetecciÃ³n y reconocimiento de rostros

```python
import face_recognition
import cv2
import numpy as np

class FaceRecognitionSystem:
    def __init__(self):
        self.known_face_encodings = []
        self.known_face_names = []
        
    def add_person(self, image_path, name):
        """AÃ±ade una persona a la base de datos"""
        image = face_recognition.load_image_file(image_path)
        encodings = face_recognition.face_encodings(image)
        
        if len(encodings) > 0:
            self.known_face_encodings.append(encodings[0])
            self.known_face_names.append(name)
            return True
        return False
    
    def recognize_faces(self, frame):
        """Reconoce rostros en un frame"""
        # Reducir tamaÃ±o para procesamiento mÃ¡s rÃ¡pido
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        # Encontrar rostros
        face_locations = face_recognition.face_locations(rgb_small_frame)
        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)
        
        face_names = []
        for face_encoding in face_encodings:
            # Comparar con rostros conocidos
            matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding)
            name = "Desconocido"
            
            # Usar el rostro con menor distancia
            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)
            if len(face_distances) > 0:
                best_match_index = np.argmin(face_distances)
                if matches[best_match_index]:
                    name = self.known_face_names[best_match_index]
            
            face_names.append(name)
        
        # Escalar de vuelta las ubicaciones
        face_locations = [(top*4, right*4, bottom*4, left*4) 
                         for (top, right, bottom, left) in face_locations]
        
        return face_locations, face_names
```

### DetecciÃ³n de emociones

```python
from deepface import DeepFace

class EmotionDetector:
    def __init__(self):
        self.emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']
    
    def detect_emotions(self, frame):
        """Detecta emociones en rostros"""
        try:
            result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)
            
            if isinstance(result, list):
                emotions_detected = []
                for face in result:
                    emotions_detected.append({
                        'region': face['region'],
                        'dominant_emotion': face['dominant_emotion'],
                        'emotion_scores': face['emotion']
                    })
                return emotions_detected
            else:
                return [{
                    'region': result['region'],
                    'dominant_emotion': result['dominant_emotion'],
                    'emotion_scores': result['emotion']
                }]
        except:
            return []
```

---

## 3. ğŸ—ºï¸ SLAM Visual

### ImplementaciÃ³n con ORB-SLAM3 (ROS)

```bash
# InstalaciÃ³n
cd ~/catkin_ws/src
git clone https://github.com/UZ-SLAMLab/ORB_SLAM3.git
cd ORB_SLAM3
./build.sh

# Lanzar SLAM con cÃ¡mara monocular
rosrun ORB_SLAM3 Mono vocabulary.txt camera_config.yaml
```

### RTAB-Map (RGB-D SLAM)

```bash
# Lanzar RTAB-Map con RealSense
roslaunch realsense2_camera rs_camera.launch

roslaunch rtabmap_ros rtabmap.launch \
    rtabmap_args:="--delete_db_on_start" \
    depth_topic:=/camera/depth/image_rect_raw \
    rgb_topic:=/camera/color/image_raw \
    camera_info_topic:=/camera/color/camera_info \
    approx_sync:=false
```

---

## 4. ğŸ“ PercepciÃ³n de Profundidad

### VisiÃ³n estÃ©reo

```python
import cv2
import numpy as np

class StereoDepthEstimator:
    def __init__(self, baseline=0.06, focal_length=700):
        """
        baseline: distancia entre cÃ¡maras en metros
        focal_length: focal length en pÃ­xeles
        """
        self.baseline = baseline
        self.focal_length = focal_length
        
        # Crear matcher estÃ©reo
        self.stereo = cv2.StereoBM_create(numDisparities=16*5, blockSize=15)
        
    def compute_disparity(self, img_left, img_right):
        """Calcula mapa de disparidad"""
        gray_left = cv2.cvtColor(img_left, cv2.COLOR_BGR2GRAY)
        gray_right = cv2.cvtColor(img_right, cv2.COLOR_BGR2GRAY)
        
        disparity = self.stereo.compute(gray_left, gray_right)
        return disparity
    
    def disparity_to_depth(self, disparity):
        """Convierte disparidad a profundidad (en metros)"""
        # Evitar divisiÃ³n por cero
        disparity = disparity.astype(np.float32) / 16.0
        disparity[disparity == 0] = 0.1
        
        # Z = (baseline * focal_length) / disparity
        depth = (self.baseline * self.focal_length) / disparity
        return depth
    
    def get_point_cloud(self, disparity, img_left):
        """Genera nube de puntos 3D"""
        depth = self.disparity_to_depth(disparity)
        
        height, width = disparity.shape
        points = []
        colors = []
        
        for v in range(height):
            for u in range(width):
                if depth[v, u] > 0 and depth[v, u] < 10:  # Filtrar ruido
                    z = depth[v, u]
                    x = (u - width/2) * z / self.focal_length
                    y = (v - height/2) * z / self.focal_length
                    
                    points.append([x, y, z])
                    colors.append(img_left[v, u] / 255.0)
        
        return np.array(points), np.array(colors)
```

### Usando RealSense RGB-D

```python
import pyrealsense2 as rs
import numpy as np

class RealSenseCamera:
    def __init__(self, width=640, height=480, fps=30):
        self.pipeline = rs.pipeline()
        config = rs.config()
        
        config.enable_stream(rs.stream.depth, width, height, rs.format.z16, fps)
        config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)
        
        self.pipeline.start(config)
        
        # Align depth to color
        self.align = rs.align(rs.stream.color)
        
    def get_frames(self):
        """Obtiene frames RGB y Depth alineados"""
        frames = self.pipeline.wait_for_frames()
        aligned_frames = self.align.process(frames)
        
        depth_frame = aligned_frames.get_depth_frame()
        color_frame = aligned_frames.get_color_frame()
        
        if not depth_frame or not color_frame:
            return None, None
        
        depth_image = np.asanyarray(depth_frame.get_data())
        color_image = np.asanyarray(color_frame.get_data())
        
        return color_image, depth_image
    
    def get_3d_point(self, x, y, depth_frame):
        """Obtiene coordenadas 3D de un pÃ­xel"""
        depth_intrin = depth_frame.profile.as_video_stream_profile().intrinsics
        depth = depth_frame.get_distance(x, y)
        point_3d = rs.rs2_deproject_pixel_to_point(depth_intrin, [x, y], depth)
        return point_3d
    
    def stop(self):
        self.pipeline.stop()
```

---

## 5. ğŸ¤š EstimaciÃ³n de Pose y Gestos

### DetecciÃ³n de pose humana con MediaPipe

```python
import mediapipe as mp
import cv2

class PoseEstimator:
    def __init__(self):
        self.mp_pose = mp.solutions.pose
        self.mp_drawing = mp.solutions.drawing_utils
        self.pose = self.mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
    
    def process_frame(self, frame):
        """Procesa frame y detecta pose"""
        # Convertir a RGB
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.pose.process(image_rgb)
        
        if results.pose_landmarks:
            # Dibujar landmarks
            self.mp_drawing.draw_landmarks(
                frame,
                results.pose_landmarks,
                self.mp_pose.POSE_CONNECTIONS
            )
            
            # Extraer coordenadas de puntos clave
            landmarks = results.pose_landmarks.landmark
            keypoints = []
            for lm in landmarks:
                keypoints.append({
                    'x': lm.x,
                    'y': lm.y,
                    'z': lm.z,
                    'visibility': lm.visibility
                })
            
            return frame, keypoints
        
        return frame, None
    
    def detect_gesture(self, keypoints):
        """Detecta gestos especÃ­ficos basados en pose"""
        if not keypoints:
            return "none"
        
        # Ejemplo: detectar brazos levantados
        left_shoulder = keypoints[11]
        left_wrist = keypoints[15]
        right_shoulder = keypoints[12]
        right_wrist = keypoints[16]
        
        if left_wrist['y'] < left_shoulder['y'] and right_wrist['y'] < right_shoulder['y']:
            return "both_arms_up"
        elif left_wrist['y'] < left_shoulder['y']:
            return "left_arm_up"
        elif right_wrist['y'] < right_shoulder['y']:
            return "right_arm_up"
        
        return "neutral"
```

---

## ğŸ“Š Pipeline de VisiÃ³n Completo

```python
class VisionPipeline:
    """Pipeline completo de visiÃ³n para el robot"""
    
    def __init__(self):
        self.object_detector = ObjectDetector('yolov8n.pt')
        self.face_recognizer = FaceRecognitionSystem()
        self.emotion_detector = EmotionDetector()
        self.pose_estimator = PoseEstimator()
        self.depth_camera = RealSenseCamera()
        
    def process_frame(self, mode='full'):
        """
        Procesa frame segÃºn el modo
        mode: 'detection', 'faces', 'pose', 'full'
        """
        color_frame, depth_frame = self.depth_camera.get_frames()
        
        if color_frame is None:
            return None
        
        results = {
            'timestamp': time.time(),
            'objects': [],
            'faces': [],
            'emotions': [],
            'pose': None,
            'gesture': None
        }
        
        if mode in ['detection', 'full']:
            results['objects'] = self.object_detector.detect(color_frame)
        
        if mode in ['faces', 'full']:
            face_locations, face_names = self.face_recognizer.recognize_faces(color_frame)
            results['faces'] = list(zip(face_locations, face_names))
            results['emotions'] = self.emotion_detector.detect_emotions(color_frame)
        
        if mode in ['pose', 'full']:
            _, keypoints = self.pose_estimator.process_frame(color_frame)
            results['pose'] = keypoints
            if keypoints:
                results['gesture'] = self.pose_estimator.detect_gesture(keypoints)
        
        return results, color_frame, depth_frame
```

---

## ğŸ”§ CalibraciÃ³n de CÃ¡maras

```python
import cv2
import numpy as np
import glob

def calibrate_camera(images_path, pattern_size=(9, 6), square_size=0.025):
    """
    Calibra cÃ¡mara usando patrÃ³n de ajedrez
    pattern_size: (columnas, filas) de esquinas internas
    square_size: tamaÃ±o del cuadrado en metros
    """
    # Preparar puntos del patrÃ³n 3D
    objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)
    objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)
    objp *= square_size
    
    # Arrays para almacenar puntos
    objpoints = []  # Puntos 3D en el mundo real
    imgpoints = []  # Puntos 2D en la imagen
    
    images = glob.glob(images_path + '/*.jpg')
    
    for fname in images:
        img = cv2.imread(fname)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # Encontrar esquinas del tablero
        ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)
        
        if ret:
            objpoints.append(objp)
            imgpoints.append(corners)
    
    # Calibrar
    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(
        objpoints, imgpoints, gray.shape[::-1], None, None
    )
    
    return mtx, dist, rvecs, tvecs

# Guardar parÃ¡metros de calibraciÃ³n
def save_calibration(filename, mtx, dist):
    np.savez(filename, mtx=mtx, dist=dist)

# Cargar parÃ¡metros
def load_calibration(filename):
    data = np.load(filename)
    return data['mtx'], data['dist']
```

---

## ğŸ“š Referencias y Recursos

### Bibliotecas

```bash
# Instalar dependencias
pip install opencv-python opencv-contrib-python
pip install ultralytics  # YOLO
pip install face-recognition
pip install deepface
pip install mediapipe
pip install pyrealsense2
pip install torch torchvision
```

### Datasets recomendados
- **COCO**: Common Objects in Context (detecciÃ³n de objetos)
- **KITTI**: Autonomous driving dataset
- **ImageNet**: ClasificaciÃ³n de imÃ¡genes
- **WIDER FACE**: DetecciÃ³n de rostros
- **LFW**: Labeled Faces in the Wild
- **MPII**: Human pose estimation

### Papers importantes
1. "You Only Look Once: Unified, Real-Time Object Detection" (Redmon et al., 2016)
2. "Mask R-CNN" (He et al., 2017)
3. "ORB-SLAM3" (Campos et al., 2021)
4. "FaceNet" (Schroff et al., 2015)

---

**Ver tambiÃ©n**:
- [ğŸ§  PercepciÃ³n](../01_Percepcion/) - FusiÃ³n sensorial multimodal
- [ğŸ“ LocalizaciÃ³n y Mapeo](../02_Localizacion_Mapeo/) - SLAM
- [ğŸ¤– Aprendizaje de MÃ¡quina](../05_Aprendizaje_Maquina/) - Entrenamiento de modelos
- [ğŸ“š Recursos de Conocimiento](../00_Gestion_Proyecto/recursos_conocimientos.md)