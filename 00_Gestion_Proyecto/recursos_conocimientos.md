# üìö Recursos y Conocimientos Necesarios

## Visi√≥n General

Este documento detalla los 12 pilares de conocimiento necesarios para la construcci√≥n exitosa de un robot humanoide, con referencias a las carpetas del proyecto donde se aplican.

---

## üéØ Marco Te√≥rico: Inteligencias M√∫ltiples Aplicadas a la Rob√≥tica

La teor√≠a de las **Inteligencias M√∫ltiples** de Howard Gardner (1983) proporciona un marco conceptual valioso para dise√±ar robots humanoides con capacidades diversificadas. Este enfoque nos permite identificar qu√© competencias desarrollar en cada √°rea del robot.

### Tabla de Inteligencias M√∫ltiples en Rob√≥tica

| Inteligencia | Adaptaci√≥n en robots | Competencias clave en rob√≥tica | Ejemplos de actividades | √Åreas a mejorar |
|--------------|---------------------|--------------------------------|------------------------|-----------------|
| **Ling√º√≠stica-verbal** | Procesamiento y generaci√≥n de lenguaje natural | - Reconocimiento de voz<br>- Comprensi√≥n sem√°ntica<br>- S√≠ntesis de lenguaje natural | Asistentes conversacionales, traducci√≥n, interacci√≥n humano-robot | Mayor comprensi√≥n del **contexto**, **iron√≠a** y **emociones** en el lenguaje |
| **L√≥gico-matem√°tica** | Capacidad de razonamiento, c√°lculo y resoluci√≥n de problemas | - Algoritmos de planificaci√≥n<br>- Optimizaci√≥n<br>- Aprendizaje autom√°tico | Resoluci√≥n de problemas, navegaci√≥n aut√≥noma, diagn√≥stico | Razonamiento **abductivo** y **creatividad algor√≠tmica**, no solo l√≥gica estricta |
| **Espacial-visual** | Percepci√≥n del entorno mediante sensores y visi√≥n artificial | - Reconocimiento de objetos<br>- SLAM (mapas y localizaci√≥n)<br>- Simulaci√≥n 3D | Conducci√≥n aut√≥noma, manipulaci√≥n de objetos, drones | Mejorar la **comprensi√≥n del contexto visual** y la **anticipaci√≥n de movimientos** |
| **Musical** | An√°lisis y generaci√≥n de patrones de sonido | - Reconocimiento de ritmos<br>- Clasificaci√≥n de audio<br>- S√≠ntesis sonora | Robots m√∫sicos, asistentes creativos, reconocimiento ac√∫stico | Detecci√≥n de **emociones en la m√∫sica** y generaci√≥n m√°s **expresiva** |
| **Corporal-kinest√©sica** | Coordinaci√≥n de movimientos f√≠sicos y manipulaci√≥n | - Control de motores<br>- Coordinaci√≥n multiarticulada<br>- Interacci√≥n h√°ptica | Humanoides, cirug√≠as asistidas, brazos rob√≥ticos | Movimientos m√°s **naturales, fluidos y adaptativos** |
| **Interpersonal** | Capacidad para interactuar con humanos de manera social | - Detecci√≥n de emociones<br>- Comunicaci√≥n no verbal<br>- Colaboraci√≥n | Robots de asistencia, ense√±anza, servicio al cliente | Mayor **empat√≠a artificial** y adaptaci√≥n a **normas culturales** |
| **Intrapersonal** | Autogesti√≥n y autorregulaci√≥n interna | - Evaluaci√≥n del estado interno<br>- Planificaci√≥n aut√≥noma<br>- Gesti√≥n de prioridades | Robots aut√≥nomos que deciden cu√°ndo recargarse, priorizar tareas | Desarrollo de una **metacognici√≥n artificial** (saber lo que sabe) |
| **Naturalista** | Reconocimiento y clasificaci√≥n de entornos naturales | - Sensores ambientales<br>- Identificaci√≥n de especies<br>- An√°lisis de datos ecol√≥gicos | Robots agr√≠colas, de exploraci√≥n ambiental, conservaci√≥n | Mejorar la **interpretaci√≥n contextual** y las **relaciones ecosist√©micas** |

### Aplicaci√≥n al Proyecto de Robot Humanoide

Este marco te√≥rico gu√≠a el desarrollo de competencias en nuestro robot:

#### üìç Mapeo a las carpetas del proyecto

```
üß† Ling√º√≠stica-verbal    ‚Üí 01_Percepcion/audio/, 05_Aprendizaje_Maquina/nlp/
üî¢ L√≥gico-matem√°tica     ‚Üí 03_Planificacion/, 11_Herramientas_Utilidades/
üëÅÔ∏è Espacial-visual       ‚Üí 06_Vision/, 02_Localizacion_Mapeo/
üéµ Musical               ‚Üí 01_Percepcion/audio/, aplicaciones creativas
ü§∏ Corporal-kinest√©sica  ‚Üí 04_Control/, 08_Integracion_Hardware/
üë• Interpersonal         ‚Üí 05_Aprendizaje_Maquina/social_interaction/
ü™û Intrapersonal         ‚Üí 00_Gestion_Proyecto/, sistemas de toma de decisiones
üåø Naturalista           ‚Üí 01_Percepcion/chemical/, aplicaciones especializadas
```

#### üéØ Objetivos de desarrollo por inteligencia

**Fase 1 (Meses 1-6)**: Fundamentos
- ‚úÖ Espacial-visual: Visi√≥n b√°sica y navegaci√≥n
- ‚úÖ Corporal-kinest√©sica: Control de movimientos b√°sicos
- ‚úÖ L√≥gico-matem√°tica: Planificaci√≥n de rutas

**Fase 2 (Meses 7-9)**: Interacci√≥n
- ‚úÖ Ling√º√≠stica-verbal: Reconocimiento y s√≠ntesis de voz
- ‚úÖ Interpersonal: Detecci√≥n de emociones b√°sicas

**Fase 3 (Meses 10-12)**: Autonom√≠a
- ‚úÖ Intrapersonal: Sistema de toma de decisiones aut√≥nomo
- ‚úÖ Musical: Reconocimiento de contextos ac√∫sticos (opcional)
- ‚úÖ Naturalista: Sensores ambientales (opcional)

### Evaluaci√≥n de Inteligencias

Para medir el desarrollo de cada inteligencia en el robot:

```python
class IntelligenceEvaluator:
    """Eval√∫a el nivel de desarrollo de cada inteligencia"""
    
    def __init__(self):
        self.scores = {
            'linguistic': 0,
            'logical': 0,
            'spatial': 0,
            'musical': 0,
            'kinesthetic': 0,
            'interpersonal': 0,
            'intrapersonal': 0,
            'naturalist': 0
        }
    
    def evaluate_linguistic(self, tests):
        """
        Tests: reconocimiento de voz, comprensi√≥n de contexto, 
               generaci√≥n de respuestas coherentes
        """
        score = 0
        if tests['speech_recognition_accuracy'] > 0.9:
            score += 25
        if tests['context_understanding']:
            score += 25
        if tests['natural_response_generation']:
            score += 25
        if tests['emotion_detection_in_voice']:
            score += 25
        return score
    
    def evaluate_spatial(self, tests):
        """
        Tests: SLAM accuracy, object detection, navigation
        """
        score = 0
        if tests['object_detection_map'] > 0.8:
            score += 25
        if tests['slam_loop_closure']:
            score += 25
        if tests['autonomous_navigation_success_rate'] > 0.9:
            score += 25
        if tests['3d_reconstruction_quality'] > 0.7:
            score += 25
        return score
    
    # ... m√°s m√©todos de evaluaci√≥n para cada inteligencia
    
    def generate_report(self):
        """Genera reporte del desarrollo de inteligencias"""
        total = sum(self.scores.values())
        avg = total / len(self.scores)
        
        report = "üìä EVALUACI√ìN DE INTELIGENCIAS M√öLTIPLES\n"
        report += "=" * 50 + "\n\n"
        
        for intelligence, score in sorted(self.scores.items(), key=lambda x: x[1], reverse=True):
            bar = "‚ñà" * (score // 5) + "‚ñë" * (20 - score // 5)
            report += f"{intelligence:15s} [{bar}] {score}/100\n"
        
        report += f"\n{'Promedio Total':15s} {avg:.1f}/100\n"
        
        return report
```

### Prioridades de investigaci√≥n

**√Åreas m√°s desafiantes** (requieren investigaci√≥n adicional):

1. **Intrapersonal** (Metacognici√≥n): 
   - ¬øC√≥mo lograr que el robot "sepa lo que sabe"?
   - Modelos de uncertainty quantification
   - Self-supervised learning

2. **Interpersonal** (Empat√≠a artificial):
   - Reconocimiento de emociones multimodal
   - Adaptaci√≥n cultural
   - Teor√≠a de la mente (Theory of Mind)

3. **Ling√º√≠stica** (Comprensi√≥n profunda):
   - Pragm√°tica del lenguaje
   - Iron√≠a y sarcasmo
   - Contexto conversacional de largo plazo

### Referencias te√≥ricas

**Papers clave**:
- Gardner, H. (1983). "Frames of Mind: The Theory of Multiple Intelligences"
- Dautenhahn, K. (2007). "Socially intelligent robots: dimensions of human‚Äìrobot interaction"
- Breazeal, C. (2003). "Toward sociable robots"

**Aplicaciones en IA**:
- Multi-task learning (una red, m√∫ltiples inteligencias)
- Ensemble methods (especialistas por inteligencia)
- Curriculum learning (desarrollar inteligencias secuencialmente)

---

## 1. üî© Mec√°nica y Dise√±o Mec√°nico

**Carpeta principal**: [`08_Integracion_Hardware/`](../08_Integracion_Hardware/)

### Conocimientos Fundamentales

#### 1.1 Conceptos B√°sicos de Mec√°nica
- Principios de fuerza, movimiento, fricci√≥n y torque
- Leyes de Newton y mec√°nica newtoniana
- Est√°tica y din√°mica de cuerpos r√≠gidos
- An√°lisis de cargas y distribuci√≥n de fuerzas

#### 1.2 Dise√±o de Articulaciones y Estructuras
- Dise√±o de articulaciones humanoides (rodillas, codos, caderas, hombros)
- Rango de movimiento (ROM) similar al humano
- Estructuras resistentes y ligeras
- Optimizaci√≥n peso-resistencia

#### 1.3 Modelado 3D y CAD
**Software recomendado**:
- SolidWorks (profesional)
- Fusion 360 (intermedio)
- FreeCAD (open source)
- Blender (visualizaci√≥n)

**Habilidades**:
- Modelado param√©trico
- Ensamblajes y restricciones
- An√°lisis de interferencias
- Generaci√≥n de planos t√©cnicos

#### 1.4 Selecci√≥n de Materiales
**Materiales comunes**:
- **Pl√°sticos**: ABS, PLA, PETG, Nylon
  - Ventajas: Livianos, econ√≥micos, f√°cil fabricaci√≥n
  - Uso: Carcasas, soportes, piezas no estructurales
  
- **Metales**: Aluminio, acero, titanio
  - Ventajas: Alta resistencia, durabilidad
  - Uso: Estructura principal, ejes, elementos de carga
  
- **Compuestos**: Fibra de carbono, fibra de vidrio
  - Ventajas: Excelente relaci√≥n peso-resistencia
  - Uso: Componentes estructurales cr√≠ticos

#### 1.5 Cinem√°tica y Din√°mica
- **Cinem√°tica directa**: Posici√≥n final a partir de √°ngulos articulares
- **Cinem√°tica inversa**: √Ångulos necesarios para posici√≥n deseada
- **Din√°mica**: Fuerzas y torques necesarios para movimiento
- **Ecuaciones de Denavit-Hartenberg**

#### 1.6 Simulaci√≥n y Pruebas Virtuales
**Software**:
- Gazebo (simulaci√≥n rob√≥tica)
- PyBullet (f√≠sica)
- V-REP/CoppeliaSim
- MATLAB/Simulink

**Pruebas**:
- An√°lisis de elementos finitos (FEA)
- Simulaci√≥n de movimiento
- Detecci√≥n de colisiones
- An√°lisis modal

#### 1.7 Tolerancias y Ajustes
- Tolerancias dimensionales (ISO 286)
- Ajustes deslizantes vs. ajustes por presi√≥n
- Holguras en articulaciones
- Control de calidad dimensional

#### 1.8 Fabricaci√≥n y Ensamblaje
**T√©cnicas**:
- Impresi√≥n 3D (FDM, SLA, SLS)
- Mecanizado CNC
- Corte l√°ser
- Doblado y conformado

**Habilidades de ensamblaje**:
- Lectura de planos
- Uso de herramientas manuales
- T√©cnicas de sujeci√≥n (tornillos, adhesivos, remaches)
- Verificaci√≥n de alineaci√≥n

### Referencias y Recursos
- üìñ "Robot Modeling and Control" - Spong, Hutchinson, Vidyasagar
- üìñ "Introduction to Robotics: Mechanics and Control" - John J. Craig
- üéì Cursos: Coursera "Modern Robotics"
- üõ†Ô∏è Tutoriales: GrabCAD, Thingiverse

---

## 2. ‚ö° Electr√≥nica y Electricidad

**Carpeta principal**: [`08_Integracion_Hardware/`](../08_Integracion_Hardware/)

### Conocimientos Fundamentales

#### 2.1 Fundamentos de Electricidad
- **Ley de Ohm**: V = I √ó R
- **Leyes de Kirchhoff**: Corriente y voltaje
- Potencia el√©ctrica: P = V √ó I
- Circuitos serie y paralelo
- AC vs DC

#### 2.2 Componentes Electr√≥nicos

**Componentes pasivos**:
- Resistencias: limitaci√≥n de corriente
- Condensadores: filtrado, almacenamiento energ√≠a
- Inductores: filtrado, almacenamiento magn√©tico
- Diodos: rectificaci√≥n, protecci√≥n

**Componentes activos**:
- Transistores (BJT, MOSFET): conmutaci√≥n, amplificaci√≥n
- Reguladores de voltaje: LM7805, LM317, buck/boost
- Optoacopladores: aislamiento
- Amplificadores operacionales

#### 2.3 Dise√±o de Circuitos
**Software de dise√±o**:
- KiCad (open source)
- Eagle/Fusion 360 Electronics
- EasyEDA
- Altium Designer (profesional)

**Proceso**:
1. Esquem√°tico
2. Selecci√≥n de componentes
3. Layout de PCB
4. Verificaci√≥n de reglas (DRC)
5. Generaci√≥n de Gerbers

#### 2.4 Microcontroladores y Microprocesadores

**Plataformas comunes**:
- **Arduino**: F√°cil, comunidad grande
  - Arduino Mega 2560 (muchos I/O)
  - Arduino Due (32-bit, m√°s potente)
  
- **Raspberry Pi**: Computadora completa
  - RPi 4 (8GB RAM recomendado)
  - Puede correr ROS
  
- **Nvidia Jetson**: IA y visi√≥n computacional
  - Jetson Nano (econ√≥mico)
  - Jetson Xavier (alto rendimiento)
  
- **ESP32**: WiFi/Bluetooth integrado
- **STM32**: Alto rendimiento, bajo consumo
- **Teensy**: Tiempo real, muchos perif√©ricos

#### 2.5 Comunicaci√≥n y Protocolos

**Protocolos comunes**:
- **UART/Serial**: Comunicaci√≥n simple
- **I2C**: M√∫ltiples dispositivos, 2 cables
- **SPI**: Alta velocidad, m√°s cables
- **CAN**: Robusto, automotriz
- **Ethernet/WiFi**: Red
- **USB**: Perif√©ricos est√°ndar

#### 2.6 Control de Motores

**Tipos de motores**:
- **Servomotores**: Control de posici√≥n preciso
  - Anal√≥gicos vs digitales
  - PWM para control
  
- **Motores DC**: Control de velocidad
  - Brushed vs brushless
  - Controladores H-bridge (L298N, BTS7960)
  
- **Motores paso a paso**: Posicionamiento exacto
  - Unipolares vs bipolares
  - Drivers A4988, DRV8825

**Controladores recomendados**:
- PCA9685: 16 servos PWM
- Adafruit Motor Shield
- ODrive: Control BLDC avanzado

#### 2.7 Alimentaci√≥n y Energ√≠a
- C√°lculo de consumo energ√©tico
- Selecci√≥n de bater√≠as (LiPo, Li-Ion)
- Regulaci√≥n de voltaje (linear vs switching)
- Distribuci√≥n de energ√≠a
- Protecci√≥n contra sobrecorriente
- BMS (Battery Management System)

#### 2.8 Detecci√≥n y Acondicionamiento de Se√±ales

**Sensores comunes**:
- IMU (MPU6050, BNO055): Orientaci√≥n
- Encoder: Posici√≥n/velocidad
- Sensor de corriente (INA219): Monitoreo
- Sensores de fuerza (FSR, c√©lula de carga)
- Sensores de proximidad (ultras√≥nico, IR)

**Acondicionamiento**:
- Filtrado de ruido
- Amplificaci√≥n
- Conversi√≥n ADC/DAC
- Calibraci√≥n

#### 2.9 Soldadura y Montaje

**T√©cnicas**:
- Soldadura con esta√±o
- Soldadura SMD (superficie)
- Hot air rework
- Crimping de conectores

**Herramientas**:
- Estaci√≥n de soldadura
- Mult√≠metro
- Osciloscopio
- Fuente de alimentaci√≥n

#### 2.10 Seguridad El√©ctrica
- Manejo seguro de voltajes
- Protecci√≥n contra cortocircuitos
- Fusibles y circuit breakers
- Aislamiento y conexi√≥n a tierra
- Normas de seguridad (IEC, UL)

### Referencias y Recursos
- üìñ "The Art of Electronics" - Horowitz & Hill
- üìñ "Practical Electronics for Inventors" - Scherz & Monk
- üéì Cursos: edX "Circuits and Electronics"
- üåê Sitios: All About Circuits, Electronics Tutorials

---

## 3. üíª Programaci√≥n y Control

**Carpetas principales**: 
- [`04_Control/`](../04_Control/)
- [`03_Planificacion/`](../03_Planificacion/)

### Conocimientos Fundamentales

#### 3.1 Lenguajes de Programaci√≥n

**Python** üêç
- Uso: Prototipado r√°pido, IA/ML, scripts
- Librer√≠as: NumPy, SciPy, Matplotlib
- Frameworks: ROS, PyBullet

**C++** ‚öôÔ∏è
- Uso: Control en tiempo real, alto rendimiento
- Librer√≠as: Eigen, Boost
- ROS nativo

**C** 
- Uso: Microcontroladores, firmware
- Arduino, STM32

**MATLAB/Octave**
- Uso: Simulaci√≥n, prototipado
- Toolboxes: Robotics, Control Systems

#### 3.2 Desarrollo de Software de Control

**Arquitectura de software**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Interfaz de Usuario       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Planificaci√≥n de Alto     ‚îÇ
‚îÇ   Nivel (Decisiones)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Control de Movimiento     ‚îÇ
‚îÇ   (Cinem√°tica)              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Control de Bajo Nivel     ‚îÇ
‚îÇ   (PID, Torque)             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Drivers de Hardware       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 3.3 Control de Movimiento

**Cinem√°tica Inversa**:
```python
# Pseudoc√≥digo
def inverse_kinematics(target_position, target_orientation):
    """
    Calcula √°ngulos articulares para alcanzar posici√≥n objetivo
    """
    joint_angles = solve_ik(target_position, target_orientation)
    return joint_angles
```

**M√©todos**:
- M√©todo anal√≠tico (cuando es posible)
- M√©todo num√©rico (Jacobiano)
- M√©todo geom√©trico

#### 3.4 Controladores

**PID (Proporcional-Integral-Derivativo)**:
```python
class PIDController:
    def __init__(self, Kp, Ki, Kd):
        self.Kp = Kp  # Ganancia proporcional
        self.Ki = Ki  # Ganancia integral
        self.Kd = Kd  # Ganancia derivativa
        self.integral = 0
        self.prev_error = 0
    
    def update(self, setpoint, measurement, dt):
        error = setpoint - measurement
        self.integral += error * dt
        derivative = (error - self.prev_error) / dt
        output = (self.Kp * error + 
                 self.Ki * self.integral + 
                 self.Kd * derivative)
        self.prev_error = error
        return output
```

**Otros controladores**:
- LQR (Linear Quadratic Regulator)
- MPC (Model Predictive Control)
- Control adaptativo
- Control robusto

#### 3.5 ROS (Robot Operating System)

**Conceptos clave**:
- **Nodos**: Procesos independientes
- **Topics**: Canales de comunicaci√≥n
- **Servicios**: Llamadas RPC
- **Acciones**: Tareas de larga duraci√≥n
- **Par√°metros**: Configuraci√≥n

**Ejemplo b√°sico**:
```python
import rospy
from std_msgs.msg import String

def talker():
    pub = rospy.Publisher('chatter', String, queue_size=10)
    rospy.init_node('talker', anonymous=True)
    rate = rospy.Rate(10)  # 10hz
    while not rospy.is_shutdown():
        hello_str = "hello world %s" % rospy.get_time()
        pub.publish(hello_str)
        rate.sleep()
```

#### 3.6 Interfaz de Usuario

**GUI Frameworks**:
- Qt/PyQt: Desktop, profesional
- Tkinter: Python, simple
- Web-based: Flask/Django + JavaScript
- ROS Web: roslibjs

**Visualizaci√≥n**:
- RViz: Visualizaci√≥n ROS
- Gazebo: Simulaci√≥n 3D
- Matplotlib: Gr√°ficas
- Plotly/Dash: Web interactivo

#### 3.7 Integraci√≥n de Sensores

**Pipeline de procesamiento**:
```
Sensor ‚Üí Driver ‚Üí Filtrado ‚Üí Fusi√≥n ‚Üí Control
```

**Ejemplo lectura IMU**:
```python
import smbus
import math

class IMU:
    def __init__(self, address=0x68):
        self.bus = smbus.SMBus(1)
        self.address = address
        self.init_mpu()
    
    def read_accel(self):
        accel_x = self.read_word_2c(0x3B)
        accel_y = self.read_word_2c(0x3D)
        accel_z = self.read_word_2c(0x3F)
        return (accel_x, accel_y, accel_z)
```

#### 3.8 Planificaci√≥n de Movimiento

**Algoritmos**:
- **RRT (Rapidly-exploring Random Trees)**: Exploraci√≥n espacial
- **A***: B√∫squeda en grafos
- **Dijkstra**: Camino m√°s corto
- **Potential Fields**: Navegaci√≥n reactiva

#### 3.9 L√≥gica de Comportamiento

**M√°quinas de estado**:
```python
from enum import Enum

class RobotState(Enum):
    IDLE = 1
    WALKING = 2
    GRASPING = 3
    TALKING = 4

class BehaviorController:
    def __init__(self):
        self.state = RobotState.IDLE
    
    def update(self, inputs):
        if self.state == RobotState.IDLE:
            if inputs['start_button']:
                self.state = RobotState.WALKING
        # ... m√°s transiciones
```

#### 3.10 Depuraci√≥n y Optimizaci√≥n

**Herramientas**:
- GDB: Depurador C/C++
- pdb: Depurador Python
- Valgrind: Detecci√≥n de memory leaks
- Profilers: cProfile, gprof

**Mejores pr√°cticas**:
- Logging estructurado
- Unit testing
- Continuous Integration
- Code review

### Referencias y Recursos
- üìñ "Programming Robots with ROS" - Quigley, Gerkey, Smart
- üìñ "Modern Robotics" - Lynch & Park
- üéì ROS Tutorials: wiki.ros.org
- üåê GitHub: Awesome Robotics

---

## 4. ü§ñ Mecatr√≥nica

**Carpeta principal**: [`08_Integracion_Hardware/`](../08_Integracion_Hardware/)

### Conocimientos Fundamentales

#### 4.1 Integraci√≥n Mec√°nica-Electr√≥nica

**Dise√±o para integraci√≥n**:
- Espacios para cableado
- Accesibilidad para mantenimiento
- Protecci√≥n de componentes electr√≥nicos
- Gesti√≥n t√©rmica
- EMI/EMC (interferencia electromagn√©tica)

#### 4.2 Sistemas de Transmisi√≥n

**Opciones de transmisi√≥n**:
- **Engranajes**: 
  - Rectos, helicoidales, c√≥nicos
  - Reductores planetarios
  - Relaciones de transmisi√≥n
  
- **Correas y Poleas**:
  - Correas dentadas (GT2)
  - C√°lculo de tensi√≥n
  
- **Cables y Tendones**:
  - Actuaci√≥n remota
  - Routing de cables
  
- **Tornillos de potencia**:
  - Movimiento lineal
  - Ball screws

#### 4.3 Actuadores vs Sensores

**Integraci√≥n**:
- Coubicaci√≥n de encoder con motor
- Sensores de l√≠mite (end-stops)
- Sensores de fuerza en articulaciones
- Retroalimentaci√≥n t√°ctil

#### 4.4 Dise√±o de PCB Integrado

**Consideraciones**:
- Montaje en estructura mec√°nica
- Conectores robustos
- Protecci√≥n contra vibraci√≥n
- Disipaci√≥n de calor

#### 4.5 Cableado y Routing

**Mejores pr√°cticas**:
- Cable management
- Strain relief
- Cables flexibles para articulaciones
- Etiquetado claro
- Esquemas de cableado

#### 4.6 Sistemas de Refrigeraci√≥n

**M√©todos**:
- Ventilaci√≥n pasiva
- Ventiladores activos
- Disipadores de calor
- Heat pipes
- Espaciado adecuado de componentes

#### 4.7 Interfaz Mec√°nica-Electr√≥nica

**Montajes comunes**:
- Encoder en eje de motor
- Servo bracket
- Sensor mount
- PCB standoffs

#### 4.8 Pruebas de Integraci√≥n

**Verificaciones**:
- Rango de movimiento sin colisiones
- Acceso a todos los conectores
- Funcionamiento de sistemas de enfriamiento
- Routing de cables sin tensi√≥n excesiva

### Referencias y Recursos
- üìñ "Mechatronics: Electronic Control Systems in Mechanical Engineering" - Bolton
- üìñ "Introduction to Mechatronic Design" - Carryer, Ohline, Kenny

---

## 5. üíæ Dise√±o de Software

**Carpeta principal**: [`09_Comunicaciones_Interfaces/`](../09_Comunicaciones_Interfaces/)

### Conocimientos Fundamentales

#### 5.1 Arquitectura de Software

**Patrones de dise√±o**:
- **MVC** (Model-View-Controller)
- **Observer**: Notificaciones de eventos
- **Strategy**: Algoritmos intercambiables
- **Factory**: Creaci√≥n de objetos
- **Singleton**: Instancia √∫nica

**Arquitectura modular**:
```
robot_software/
‚îú‚îÄ‚îÄ core/           # Funcionalidad central
‚îú‚îÄ‚îÄ drivers/        # Drivers de hardware
‚îú‚îÄ‚îÄ perception/     # Procesamiento sensorial
‚îú‚îÄ‚îÄ control/        # Controladores
‚îú‚îÄ‚îÄ planning/       # Planificaci√≥n
‚îú‚îÄ‚îÄ interfaces/     # GUI, API
‚îî‚îÄ‚îÄ utils/          # Utilidades
```

#### 5.2 Programaci√≥n Orientada a Objetos

**Conceptos clave**:
```python
class Joint:
    """Clase base para articulaciones"""
    def __init__(self, name, min_angle, max_angle):
        self.name = name
        self.min_angle = min_angle
        self.max_angle = max_angle
        self.current_angle = 0
    
    def move_to(self, target_angle):
        if self.min_angle <= target_angle <= self.max_angle:
            self.current_angle = target_angle
            return True
        return False

class RevoluteJoint(Joint):
    """Articulaci√≥n rotacional"""
    def __init__(self, name, min_angle, max_angle, motor_id):
        super().__init__(name, min_angle, max_angle)
        self.motor_id = motor_id
```

#### 5.3 Desarrollo de GUI

**Qt/PyQt ejemplo**:
```python
from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton
from PyQt5.QtCore import QTimer

class RobotControlGUI(QMainWindow):
    def __init__(self):
        super().__init__()
        self.init_ui()
    
    def init_ui(self):
        self.setWindowTitle('Robot Control')
        
        start_btn = QPushButton('Start', self)
        start_btn.clicked.connect(self.start_robot)
        
        # Timer para actualizar UI
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_display)
        self.timer.start(100)  # 10 Hz
    
    def start_robot(self):
        # C√≥digo de inicio
        pass
    
    def update_display(self):
        # Actualizar visualizaci√≥n
        pass
```

#### 5.4 APIs y Comunicaci√≥n

**REST API ejemplo (Flask)**:
```python
from flask import Flask, jsonify, request

app = Flask(__name__)

@app.route('/robot/move', methods=['POST'])
def move_robot():
    data = request.json
    x = data['x']
    y = data['y']
    z = data['z']
    
    # Mover robot
    result = robot.move_to(x, y, z)
    
    return jsonify({'success': result})

@app.route('/robot/status', methods=['GET'])
def get_status():
    status = robot.get_status()
    return jsonify(status)
```

#### 5.5 Manejo de Errores

**Estrategias**:
```python
class RobotException(Exception):
    """Excepci√≥n base para errores del robot"""
    pass

class MotorException(RobotException):
    """Error en motor"""
    pass

def safe_move(joint, angle):
    try:
        joint.move_to(angle)
    except MotorException as e:
        logger.error(f"Motor error: {e}")
        robot.emergency_stop()
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise
```

#### 5.6 Logging y Debugging

**Configuraci√≥n de logging**:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('robot.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

logger.info("Robot initialized")
logger.warning("Battery low")
logger.error("Motor failure")
```

#### 5.7 Testing

**Unit testing**:
```python
import unittest

class TestJoint(unittest.TestCase):
    def setUp(self):
        self.joint = Joint("test_joint", -90, 90)
    
    def test_move_within_range(self):
        result = self.joint.move_to(45)
        self.assertTrue(result)
        self.assertEqual(self.joint.current_angle, 45)
    
    def test_move_out_of_range(self):
        result = self.joint.move_to(100)
        self.assertFalse(result)

if __name__ == '__main__':
    unittest.main()
```

#### 5.8 Documentaci√≥n de C√≥digo

**Docstrings**:
```python
def calculate_inverse_kinematics(target_pos, target_orient, robot_model):
    """
    Calcula la cinem√°tica inversa para alcanzar una posici√≥n objetivo.
    
    Args:
        target_pos (tuple): Posici√≥n objetivo (x, y, z) en metros
        target_orient (tuple): Orientaci√≥n objetivo (roll, pitch, yaw) en radianes
        robot_model (RobotModel): Modelo del robot
    
    Returns:
        list: √Ångulos articulares en radianes
        
    Raises:
        IKException: Si no se encuentra soluci√≥n
        
    Example:
        >>> angles = calculate_inverse_kinematics((0.3, 0.2, 0.5), (0, 0, 0), robot)
        >>> print(angles)
        [0.52, -0.78, 1.23, 0.45, -0.32, 0.67]
    """
    # Implementaci√≥n
    pass
```

### Referencias y Recursos
- üìñ "Clean Code" - Robert C. Martin
- üìñ "Design Patterns" - Gang of Four
- üéì Real Python, PyQt documentation

---

## 6. üß† Inteligencia Artificial y Aprendizaje Autom√°tico

**Carpetas principales**: 
- [`05_Aprendizaje_Maquina/`](../05_Aprendizaje_Maquina/)
- [`06_Vision/`](../06_Vision/)

### Conocimientos Fundamentales

#### 6.1 Conceptos de IA

**Tipos de IA**:
- **IA reactiva**: Responde a est√≠mulos
- **Memoria limitada**: Usa experiencia reciente
- **Teor√≠a de la mente**: Comprende emociones (objetivo)
- **Autoconciencia**: Conciencia propia (futuro lejano)

#### 6.2 Aprendizaje Autom√°tico

**Paradigmas**:

**Supervisado**:
- Clasificaci√≥n (SVM, Random Forest, Neural Networks)
- Regresi√≥n (Linear, Polynomial, Neural Networks)
- Aplicaciones: Reconocimiento de objetos, predicci√≥n

**No supervisado**:
- Clustering (K-means, DBSCAN, Hierarchical)
- Reducci√≥n dimensionalidad (PCA, t-SNE)
- Aplicaciones: Segmentaci√≥n, detecci√≥n de anomal√≠as

**Por refuerzo**:
- Q-Learning
- Deep Q-Networks (DQN)
- Policy Gradient
- Actor-Critic
- Aplicaciones: Control de locomoci√≥n, navegaci√≥n

#### 6.3 Redes Neuronales

**Arquitecturas**:

**Feedforward** (MLP):
```python
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x
```

**Convolucionales** (CNN):
- Aplicaci√≥n: Visi√≥n computacional
- Capas: Conv2D, MaxPool, Flatten, Dense

**Recurrentes** (RNN, LSTM, GRU):
- Aplicaci√≥n: Secuencias, predicci√≥n
- Memoria de estados anteriores

**Transformers**:
- Atenci√≥n: Enfoque en partes relevantes
- Aplicaci√≥n: NLP, visi√≥n

#### 6.4 Deep Learning Frameworks

**PyTorch**:
```python
import torch
import torch.optim as optim

model = SimpleNN(input_size=784, hidden_size=128, num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Entrenamiento
for epoch in range(num_epochs):
    for images, labels in train_loader:
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**TensorFlow/Keras**:
```python
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5, validation_split=0.2)
```

#### 6.5 Visi√≥n por Computadora

**OpenCV b√°sico**:
```python
import cv2
import numpy as np

# Cargar imagen
img = cv2.imread('image.jpg')

# Convertir a espacio de color
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

# Detecci√≥n de bordes
edges = cv2.Canny(img, 100, 200)

# Detecci√≥n de contornos
contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

# Dibujar contornos
cv2.drawContours(img, contours, -1, (0,255,0), 2)
```

**Detecci√≥n de objetos**:
- **YOLO (You Only Look Once)**: R√°pido, tiempo real
- **SSD (Single Shot Detector)**: Balance velocidad-precisi√≥n
- **Faster R-CNN**: Alta precisi√≥n
- **EfficientDet**: Eficiente

**Ejemplo YOLO con OpenCV**:
```python
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
layer_names = net.getLayerNames()
output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]

blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
net.setInput(blob)
outs = net.forward(output_layers)

# Procesar detecciones
for out in outs:
    for detection in out:
        scores = detection[5:]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence > 0.5:
            # Objeto detectado
            pass
```

#### 6.6 Procesamiento de Lenguaje Natural

**Bibliotecas**:
- NLTK: Procesamiento b√°sico
- spaCy: Producci√≥n, r√°pido
- Transformers (Hugging Face): Modelos pre-entrenados

**Reconocimiento de voz**:
```python
import speech_recognition as sr

recognizer = sr.Recognizer()

with sr.Microphone() as source:
    print("Habla ahora...")
    audio = recognizer.listen(source)

try:
    text = recognizer.recognize_google(audio, language='es-ES')
    print(f"Dijiste: {text}")
except:
    print("No pude entender")
```

**S√≠ntesis de voz (TTS)**:
```python
import pyttsx3

engine = pyttsx3.init()
engine.setProperty('rate', 150)
engine.setProperty('volume', 0.9)

engine.say("Hola, soy un robot humanoide")
engine.runAndWait()
```

#### 6.7 Aprendizaje por Refuerzo

**Conceptos**:
- **Estado** (s): Situaci√≥n actual
- **Acci√≥n** (a): Lo que el agente puede hacer
- **Recompensa** (r): Feedback del entorno
- **Pol√≠tica** (œÄ): Estrategia de decisi√≥n
- **Valor** (V): Calidad de un estado

**Implementaci√≥n simple Q-Learning**:
```python
import numpy as np

class QLearningAgent:
    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.q_table = np.zeros((n_states, n_actions))
        self.alpha = alpha  # Learning rate
        self.gamma = gamma  # Discount factor
        self.epsilon = epsilon  # Exploration rate
    
    def choose_action(self, state):
        if np.random.random() < self.epsilon:
            return np.random.randint(len(self.q_table[state]))
        return np.argmax(self.q_table[state])
    
    def learn(self, state, action, reward, next_state):
        predict = self.q_table[state, action]
        target = reward + self.gamma * np.max(self.q_table[next_state])
        self.q_table[state, action] += self.alpha * (target - predict)
```

#### 6.8 Transfer Learning

**Uso de modelos pre-entrenados**:
```python
from torchvision import models

# Cargar ResNet pre-entrenado
resnet = models.resnet50(pretrained=True)

# Congelar capas
for param in resnet.parameters():
    param.requires_grad = False

# Reemplazar √∫ltima capa
resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)

# Solo entrenar nueva capa
optimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)
```

### Referencias y Recursos
- üìñ "Deep Learning" - Goodfellow, Bengio, Courville
- üìñ "Hands-On Machine Learning" - Aur√©lien G√©ron
- üìñ "Reinforcement Learning: An Introduction" - Sutton & Barto
- üéì Coursera: Deep Learning Specialization (Andrew Ng)
- üéì Fast.ai courses
- üåê Papers With Code

---

## 7. üè≠ Materiales y Fabricaci√≥n

**Carpeta principal**: [`08_Integracion_Hardware/`](../08_Integracion_Hardware/)

### Conocimientos Fundamentales

#### 7.1 Propiedades de Materiales

**Propiedades mec√°nicas**:
- **Resistencia tensil**: Fuerza antes de ruptura
- **M√≥dulo de Young**: Rigidez
- **Elongaci√≥n**: Deformaci√≥n antes de ruptura
- **Dureza**: Resistencia a penetraci√≥n
- **Tenacidad**: Resistencia a fractura
- **Fatiga**: Resistencia a ciclos de carga

**Propiedades f√≠sicas**:
- Densidad (kg/m¬≥)
- Conductividad t√©rmica
- Coeficiente de expansi√≥n t√©rmica
- Conductividad el√©ctrica

#### 7.2 Pl√°sticos

**Termopl√°sticos** (re-moldeables):

**PLA** (Poli√°cido L√°ctico):
- ‚úÖ F√°cil impresi√≥n, biodegradable
- ‚ùå Fr√°gil, baja temperatura
- Uso: Prototipos, carcasas no estructurales

**ABS** (Acrilonitrilo Butadieno Estireno):
- ‚úÖ Resistente, flexible, mecanizable
- ‚ùå Requiere cama caliente, vapores
- Uso: Carcasas, partes funcionales

**PETG** (Tereftalato de Polietileno Glicol):
- ‚úÖ Fuerte, flexible, resistente qu√≠micamente
- ‚ùå Puede ser pegajoso
- Uso: Partes funcionales, protecci√≥n

**Nylon** (Poliamida):
- ‚úÖ Muy resistente, duradero, flexible
- ‚ùå Absorbe humedad, dif√≠cil impresi√≥n
- Uso: Engranajes, partes m√≥viles

**TPU** (Poliuretano Termopl√°stico):
- ‚úÖ Flexible, el√°stico, resistente abrasi√≥n
- ‚ùå Lenta impresi√≥n
- Uso: Juntas, amortiguadores

**Termofijos** (no re-moldeables):
- Resina epoxi
- Poliuretano
- Fibra de vidrio

#### 7.3 Metales

**Aluminio**:
- **Aleaci√≥n 6061-T6**: Uso general
- ‚úÖ Ligero (2.7 g/cm¬≥), resistente corrosi√≥n
- ‚ùå Menor resistencia que acero
- Uso: Estructura, chasis, soportes

**Acero**:
- **Acero al carbono**: Econ√≥mico, fuerte
- **Acero inoxidable 304**: Resistente corrosi√≥n
- ‚úÖ Muy resistente
- ‚ùå Pesado (7.85 g/cm¬≥)
- Uso: Ejes, pernos, partes cr√≠ticas

**Titanio**:
- ‚úÖ Alta resistencia-peso, resistente corrosi√≥n
- ‚ùå Costoso, dif√≠cil mecanizar
- Uso: Aplicaciones avanzadas

**Lat√≥n/Bronce**:
- ‚úÖ Buena maquinabilidad, baja fricci√≥n
- Uso: Bujes, conectores

#### 7.4 Materiales Compuestos

**Fibra de Carbono**:
- ‚úÖ Excelente relaci√≥n resistencia-peso
- ‚ùå Costoso, fr√°gil a impactos
- Uso: Estructura ligera y r√≠gida

**Fibra de Vidrio**:
- ‚úÖ Resistente, econ√≥mico
- ‚ùå M√°s pesado que fibra carbono
- Uso: Carcasas, paneles

#### 7.5 T√©cnicas de Fabricaci√≥n

#### Impresi√≥n 3D

**FDM** (Fused Deposition Modeling):
- Proceso: Extrusi√≥n de filamento termopl√°stico
- Materiales: PLA, ABS, PETG, Nylon, TPU
- Ventajas: Econ√≥mico, accesible
- Desventajas: L√≠neas de capa visibles, anisotrop√≠a
- Configuraciones clave:
  - Temperatura nozzle/cama
  - Velocidad impresi√≥n
  - Altura de capa (0.1-0.3mm t√≠pico)
  - Infill (10-100%)

**SLA** (Stereolithography):
- Proceso: Curado de resina con l√°ser/UV
- Ventajas: Alta resoluci√≥n, superficies lisas
- Desventajas: Materiales limitados, post-procesado
- Uso: Detalles finos, prototipos

**SLS** (Selective Laser Sintering):
- Proceso: Sinterizaci√≥n de polvo con l√°ser
- Materiales: Nylon, metales
- Ventajas: No requiere soportes, resistente
- Desventajas: Costoso, textura granular

#### Mecanizado CNC

**Fresado CNC**:
- Elimina material con herramientas rotatorias
- Ejes: 3, 4, o 5 ejes
- Tolerancias: ¬±0.01mm t√≠pico
- Materiales: Metales, pl√°sticos, madera

**Torneado CNC**:
- Piezas cil√≠ndricas
- Alta precisi√≥n en di√°metros

**G-code b√°sico**:
```gcode
G21         ; Unidades mm
G90         ; Posicionamiento absoluto
G00 X10 Y10 ; Movimiento r√°pido a (10,10)
G01 Z-2 F100; Movimiento lineal a Z=-2, velocidad 100mm/min
G02 X20 Y20 I5 J5 F200  ; Arco horario
M30         ; Fin programa
```

#### Corte L√°ser

- Materiales: Madera, acr√≠lico, cart√≥n, algunos metales
- Precisi√≥n alta
- Bordes limpios
- 2D principalmente

#### Otros Procesos

**Moldeo por Inyecci√≥n**:
- Producci√≥n en masa de pl√°sticos
- Requiere molde (costoso)
- Bajo costo unitario en volumen

**Fundici√≥n**:
- Metales fundidos en molde
- Sand casting, investment casting

**Conformado de Chapa**:
- Doblado, estampado
- Partes de metal delgado

#### 7.6 Post-Procesamiento

**Lijado**: Suavizar superficies
**Pintado**: Est√©tica, protecci√≥n
**Vapores de acetona** (ABS): Superficie lisa
**Anodizado** (aluminio): Protecci√≥n, color
**Soldadura**: Unir metales

### Referencias y Recursos
- üìñ "Manufacturing Processes for Engineering Materials" - Kalpakjian
- üåê Simplify3D Print Quality Troubleshooting Guide
- üåê MatWeb: Base de datos de materiales

---

## 8. üîã Bater√≠as y Energ√≠a

**Carpeta principal**: [`08_Integracion_Hardware/`](../08_Integracion_Hardware/)

### Conocimientos Fundamentales

#### 8.1 Tipos de Bater√≠as

**Litio-Pol√≠mero (LiPo)**:
- Voltaje nominal: 3.7V por celda
- ‚úÖ Alta densidad energ√©tica, ligera, descarga alta
- ‚ùå Requiere cuidado, riesgo incendio
- Configuraci√≥n: 2S, 3S, 4S (S = series)
- Capacidad: 1000-10000+ mAh
- C-rating: Tasa de descarga (20C = 20√ó capacidad)
- Uso: Drones, robots m√≥viles

**Litio-Ion (Li-Ion)**:
- Celdas comunes: 18650, 21700
- Voltaje nominal: 3.6-3.7V
- ‚úÖ M√°s seguro que LiPo, alta capacidad
- ‚ùå Menor tasa de descarga
- Uso: Port√°tiles, veh√≠culos el√©ctricos

**N√≠quel-Metal Hidruro (NiMH)**:
- Voltaje nominal: 1.2V por celda
- ‚úÖ Seguro, econ√≥mico
- ‚ùå Menor densidad energ√©tica, auto-descarga
- Uso: Juguetes, dispositivos legacy

#### 8.2 Especificaciones Clave

**Capacidad** (mAh, Ah):
- Cantidad de energ√≠a almacenada
- Ejemplo: 5000mAh = 5A durante 1h

**Voltaje** (V):
- LiPo: 3.7V nominal, 4.2V cargada, 3.0V descargada
- Configuraci√≥n series: n√ó3.7V
- Configuraci√≥n paralelo: misma V, suma capacidad

**C-Rating**:
- Tasa m√°xima de descarga
- Ejemplo: 5000mAh 20C = 100A m√°ximo

**Energ√≠a** (Wh):
- Wh = V √ó Ah
- Ejemplo: 11.1V (3S) √ó 5Ah = 55.5Wh

#### 8.3 C√°lculo de Autonom√≠a

```python
def calcular_autonomia(capacidad_mah, voltaje_v, potencia_watts):
    """
    Calcula autonom√≠a aproximada de bater√≠a
    
    Args:
        capacidad_mah: Capacidad bater√≠a en mAh
        voltaje_v: Voltaje nominal en V
        potencia_watts: Consumo promedio en W
    
    Returns:
        float: Tiempo en horas
    """
    capacidad_ah = capacidad_mah / 1000
    energia_wh = voltaje_v * capacidad_ah
    autonomia_h = energia_wh / potencia_watts
    return autonomia_h * 0.8  # Factor eficiencia 80%

# Ejemplo
autonomia = calcular_autonomia(5000, 11.1, 50)
print(f"Autonom√≠a: {autonomia:.2f} horas")
# Resultado: ~0.89 horas ‚âà 53 minutos
```

#### 8.4 Sistema de Gesti√≥n de Bater√≠as (BMS)

**Funciones**:
- **Protecci√≥n sobrecarga**: Detiene carga a 4.2V/celda
- **Protecci√≥n sobredescarga**: Corta a 3.0V/celda
- **Balanceo de celdas**: Ecualiza voltajes
- **Protecci√≥n sobrecorriente**: L√≠mite de A
- **Protecci√≥n sobrecalentamiento**: Monitoreo temperatura

**Ejemplo de monitoreo**:
```python
import board
import busio
from adafruit_ina219 import INA219

# Sensor corriente/voltaje
i2c = busio.I2C(board.SCL, board.SDA)
ina219 = INA219(i2c)

while True:
    voltage = ina219.bus_voltage + ina219.shunt_voltage
    current = ina219.current
    power = ina219.power
    
    print(f"Voltaje: {voltage:.2f}V")
    print(f"Corriente: {current:.2f}mA")
    print(f"Potencia: {power:.2f}mW")
    
    if voltage < 10.5:  # 3.5V/celda para 3S
        print("¬°BATER√çA BAJA!")
    
    time.sleep(1)
```

#### 8.5 Regulaci√≥n de Voltaje

**Reguladores Lineales**:
- LM7805 (5V), LM7812 (12V), LM317 (ajustable)
- ‚úÖ Simple, ruido bajo
- ‚ùå Ineficiente (disipa calor)
- Uso: Bajo consumo, voltajes fijos

**Reguladores Switching**:

**Buck** (Step-Down):
- Reduce voltaje
- Eficiencia ~90%
- Ejemplo: LM2596 (12V‚Üí5V)

**Boost** (Step-Up):
- Aumenta voltaje
- Ejemplo: MT3608 (5V‚Üí12V)

**Buck-Boost**:
- Aumenta o reduce
- Mayor flexibilidad

#### 8.6 Distribuci√≥n de Energ√≠a

**Esquema t√≠pico**:
```
Bater√≠a (11.1V LiPo 3S)
    ‚îÇ
    ‚îú‚îÄ‚Üí BMS ‚îÄ‚Üí [Protecci√≥n]
    ‚îÇ
    ‚îú‚îÄ‚Üí Buck 5V ‚îÄ‚Üí Raspberry Pi, Arduino
    ‚îÇ
    ‚îú‚îÄ‚Üí Buck 6V ‚îÄ‚Üí Servomotores
    ‚îÇ
    ‚îî‚îÄ‚Üí Directo ‚îÄ‚Üí Motores DC (con ESC)
```

**Consideraciones**:
- Cable adecuado (AWG): Mayor corriente = cable m√°s grueso
- Fusibles/breakers en cada l√≠nea
- Capacitores de desacople cerca de cada componente
- Ground com√∫n

#### 8.7 Carga de Bater√≠as

**Cargadores LiPo**:
- Balance charging: Carga cada celda individualmente
- Tasa de carga: T√≠picamente 1C (capacidad en A)
- Ejemplo: Bater√≠a 5000mAh ‚Üí cargar a 5A
- Nunca dejar sin supervisi√≥n

**Protocolo de carga**:
1. CC (Corriente Constante): Hasta 4.2V/celda
2. CV (Voltaje Constante): Corriente decae

#### 8.8 Almacenamiento y Seguridad

**Almacenamiento LiPo**:
- Voltaje: 3.8V/celda (modo storage)
- Lugar: Bolsa LiPo resistente al fuego
- Temperatura: Ambiente, seco
- Inspecci√≥n: No hinchar, da√±ar

**Seguridad**:
- ‚ö†Ô∏è Nunca sobre-descargar (<3.0V)
- ‚ö†Ô∏è Nunca sobre-cargar (>4.2V)
- ‚ö†Ô∏è Nunca perforar
- ‚ö†Ô∏è Desechar si est√° hinchada o da√±ada

#### 8.9 Alternativas de Energ√≠a

**Supercapacitores**:
- Carga/descarga muy r√°pida
- Vida √∫til muy larga
- Baja densidad energ√©tica
- Uso: Picos de potencia, backup

**Celdas de Combustible**:
- Hidr√≥geno + O2 ‚Üí electricidad + agua
- Larga duraci√≥n
- Costoso, complejo
- Uso: Aplicaciones especializadas

### Referencias y Recursos
- üìñ "Battery Management Systems" - Gregory Plett
- üåê Battery University
- üåê Oscar Liang: LiPo Battery Guide

---

## 9. üèÉ Dise√±o Ergon√≥mico y Biomec√°nica

**Carpeta principal**: [`04_Control/`](../04_Control/)

### Conocimientos Fundamentales

#### 9.1 Anatom√≠a Humana

**Articulaciones principales**:

**Pierna**:
- Cadera: 3 DOF (flexi√≥n/extensi√≥n, abducci√≥n/aducci√≥n, rotaci√≥n)
- Rodilla: 1 DOF (flexi√≥n/extensi√≥n) ~130¬∞
- Tobillo: 2 DOF (dorsiflexi√≥n/plantarflexi√≥n, inversi√≥n/eversi√≥n)

**Brazo**:
- Hombro: 3 DOF (esf√©rica) ~360¬∞
- Codo: 1 DOF (flexi√≥n) ~145¬∞
- Mu√±eca: 2 DOF (flexi√≥n/extensi√≥n, desviaci√≥n radial/ulnar)
- Mano: 20+ DOF (dedos)

**Torso**:
- Columna: Flexi√≥n, extensi√≥n, rotaci√≥n
- Cuello: 3 DOF

**DOF t√≠pico robot humanoide**:
- Cabeza: 2-3
- Cada brazo: 6-7
- Torso: 1-3
- Cada pierna: 6
- **Total**: 25-30 DOF

#### 9.2 Biomec√°nica del Movimiento

**Centro de Masa (COM)**:
- Humano: Aprox. a nivel de S2 (segunda v√©rtebra sacra)
- Cr√≠tico para equilibrio
- Debe estar dentro del pol√≠gono de soporte

**Zero Moment Point (ZMP)**:
- Punto donde no hay momento de vuelco
- Usado para locomoci√≥n estable
- Debe estar dentro de pies de soporte

**Gait (Marcha)**:
```
Ciclo de marcha humano:
1. Heel strike (impacto tal√≥n)
2. Foot flat (pie plano)
3. Mid stance (apoyo medio)
4. Heel off (despegue tal√≥n)
5. Toe off (despegue dedos)
6. Swing phase (fase de balanceo)
```

#### 9.3 Cinem√°tica Humanoide

**Denavit-Hartenberg (DH)**:
Parametrizaci√≥n est√°ndar de cadenas cinem√°ticas

```python
import numpy as np

def dh_matrix(theta, d, a, alpha):
    """
    Matriz de transformaci√≥n DH
    
    theta: √°ngulo articulaci√≥n (rad)
    d: desplazamiento en z
    a: longitud enlace
    alpha: √°ngulo de torsi√≥n
    """
    ct = np.cos(theta)
    st = np.sin(theta)
    ca = np.cos(alpha)
    sa = np.sin(alpha)
    
    return np.array([
        [ct, -st*ca, st*sa, a*ct],
        [st, ct*ca, -ct*sa, a*st],
        [0, sa, ca, d],
        [0, 0, 0, 1]
    ])

# Ejemplo: Brazo 3-DOF simple
# DH parameters: [theta, d, a, alpha]
dh_params = [
    [theta1, 0, 0, np.pi/2],
    [theta2, 0, L1, 0],
    [theta3, 0, L2, 0]
]

T = np.eye(4)
for params in dh_params:
    T = T @ dh_matrix(*params)

# T contiene la transformaci√≥n de base a end-effector
```

#### 9.4 Din√°mica de Movimiento

**Ecuaci√≥n de Euler-Lagrange**:
```
œÑ = M(q)qÃà + C(q,qÃá)qÃá + G(q)

Donde:
œÑ: Torques articulares
M(q): Matriz de inercia
C(q,qÃá): Fuerzas centr√≠fugas y Coriolis
G(q): Gravedad
q: Posiciones articulares
```

#### 9.5 Control de Equilibrio

**Estrategias**:

**Ankle Strategy**:
- Ajustes peque√±os usando tobillo
- Perturbaciones leves

**Hip Strategy**:
- Movimiento de cadera
- Perturbaciones moderadas

**Step Strategy**:
- Dar un paso
- Perturbaciones grandes

**Implementaci√≥n PID para balanceo**:
```python
class BalanceController:
    def __init__(self):
        self.pid_pitch = PIDController(Kp=10, Ki=0.1, Kd=5)
        self.pid_roll = PIDController(Kp=10, Ki=0.1, Kd=5)
    
    def update(self, imu_data, dt):
        # Leer orientaci√≥n
        pitch = imu_data['pitch']
        roll = imu_data['roll']
        
        # Calcular correcciones
        ankle_pitch = self.pid_pitch.update(0, pitch, dt)
        ankle_roll = self.pid_roll.update(0, roll, dt)
        
        return ankle_pitch, ankle_roll
```

#### 9.6 Locomoci√≥n B√≠peda

**Generaci√≥n de Trayectorias**:

**M√©todo de Captura de Movimiento (MoCap)**:
- Grabar movimiento humano real
- Adaptar a robot

**Optimizaci√≥n**:
- Minimizar energ√≠a
- Maximizar estabilidad
- Suavidad de movimiento

**Preview Control**:
- Anticipar movimientos futuros
- Planificar ZMP con antelaci√≥n

**Ejemplo simplificado de paso**:
```python
def generate_step_trajectory(start_pos, end_pos, step_height, duration, freq=100):
    """
    Genera trayectoria de un paso
    
    start_pos: (x, y, z) inicio
    end_pos: (x, y, z) fin
    step_height: altura del paso
    duration: duraci√≥n en segundos
    freq: frecuencia Hz
    """
    num_points = int(duration * freq)
    t = np.linspace(0, 1, num_points)
    
    # Interpolaci√≥n lineal en x, y
    x = start_pos[0] + (end_pos[0] - start_pos[0]) * t
    y = start_pos[1] + (end_pos[1] - start_pos[1]) * t
    
    # Trayectoria parab√≥lica en z
    z = start_pos[2] + 4 * step_height * t * (1 - t)
    
    trajectory = np.column_stack([x, y, z])
    return trajectory
```

#### 9.7 Interacci√≥n F√≠sica

**Compliance (Flexibilidad)**:
- Articulaciones no r√≠gidas
- Absorbe impactos
- M√°s seguro

**Impedance Control**:
- Control de fuerza y posici√≥n
- Permite interacci√≥n suave

```python
class ImpedanceController:
    def __init__(self, K, D):
        self.K = K  # Rigidez (stiffness)
        self.D = D  # Amortiguamiento (damping)
    
    def calculate_torque(self, pos_desired, pos_actual, vel_actual):
        error_pos = pos_desired - pos_actual
        torque = self.K * error_pos - self.D * vel_actual
        return torque
```

#### 9.8 Ergonom√≠a del Robot

**Dise√±o centrado en humanos**:
- Altura adecuada (t√≠picamente 150-180cm)
- Alcance de brazos funcional
- Interfaz natural (gestos, voz)
- Apariencia amigable

**Uncanny Valley**:
- Muy similar a humano puede ser perturbador
- Balance entre realismo y abstracci√≥n

### Referencias y Recursos
- üìñ "Humanoid Robots: Modeling and Control" - Vukobratoviƒá, Borovac
- üìñ "Biped Locomotion" - Kajita et al.
- üéì Papers: ZMP-based walking, Preview control
- üåê Open Humanoid Project

---

## 10. üì° Comunicaci√≥n y Redes

**Carpeta principal**: [`09_Comunicaciones_Interfaces/`](../09_Comunicaciones_Interfaces/)

### Conocimientos Fundamentales

#### 10.1 Protocolos de Comunicaci√≥n Inal√°mbrica

**WiFi (802.11)**:
- Alcance: 50-100m interior
- Velocidad: 54Mbps (802.11g) - 1.3Gbps (802.11ac)
- Uso: Telemetr√≠a, video streaming, control
- Frecuencias: 2.4GHz (largo alcance) y 5GHz (m√°s r√°pido)

**Configuraci√≥n WiFi en Raspberry Pi**:
```bash
# /etc/wpa_supplicant/wpa_supplicant.conf
network={
    ssid="RobotNetwork"
    psk="password123"
}

# Verificar conexi√≥n
ifconfig wlan0
ping 8.8.8.8
```

**Bluetooth/BLE**:
- Bluetooth Classic: Audio, datos (1-3Mbps)
- BLE (Low Energy): Sensores, bajo consumo
- Alcance: 10-100m
- Uso: Perif√©ricos, gamepad, sensores

**Zigbee**:
- Bajo consumo, mesh network
- Uso: Red de sensores distribuidos

**LoRa**:
- Largo alcance (km), bajo bitrate
- Uso: Telemetr√≠a remota

#### 10.2 Arquitecturas de Red

**Client-Server**:
```
Robot (Client) ‚Üê‚Üí Servidor Central
```
- Servidor hace procesamiento pesado
- Robot env√≠a datos, recibe comandos

**P2P (Peer-to-Peer)**:
```
Robot ‚Üê‚Üí Controlador
```
- Comunicaci√≥n directa
- Sin intermediarios

**Publish-Subscribe (ROS)**:
```
Nodo A (Publisher) ‚Üí Topic ‚Üí Nodo B (Subscriber)
```
- Desacoplamiento
- M√∫ltiples suscriptores

#### 10.3 Protocolos de Aplicaci√≥n

**MQTT** (Message Queue Telemetry Transport):
- Ligero, pub-sub
- Ideal para IoT

```python
import paho.mqtt.client as mqtt

def on_connect(client, userdata, flags, rc):
    print("Conectado con c√≥digo:", rc)
    client.subscribe("robot/commands")

def on_message(client, userdata, msg):
    print(f"Mensaje recibido: {msg.topic} - {msg.payload}")
    # Procesar comando

client = mqtt.Client()
client.on_connect = on_connect
client.on_message = on_message

client.connect("broker.hivemq.com", 1883, 60)
client.loop_forever()
```

**HTTP/REST API**:
- Est√°ndar web
- Request-Response

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/robot/move', methods=['POST'])
def move():
    data = request.json
    x, y, z = data['x'], data['y'], data['z']
    # Mover robot
    return jsonify({'status': 'moving', 'position': [x, y, z]})

@app.route('/robot/status', methods=['GET'])
def status():
    return jsonify({
        'battery': get_battery_level(),
        'position': get_current_position(),
        'status': 'operational'
    })

app.run(host='0.0.0.0', port=5000)
```

**WebSocket**:
- Bidireccional, tiempo real
- Ideal para control en vivo

```python
import asyncio
import websockets

async def robot_handler(websocket, path):
    async for message in websocket:
        # Procesar comando
        response = process_command(message)
        await websocket.send(response)

start_server = websockets.serve(robot_handler, "0.0.0.0", 8765)

asyncio.get_event_loop().run_until_complete(start_server)
asyncio.get_event_loop().run_forever()
```

#### 10.4 ROS Communication

**Topics** (Pub-Sub):
```python
import rospy
from std_msgs.msg import String
from sensor_msgs.msg import JointState

# Publisher
pub = rospy.Publisher('/joint_commands', JointState, queue_size=10)

# Subscriber
def callback(data):
    rospy.loginfo(f"Received: {data.data}")

sub = rospy.Subscriber('/sensor_data', String, callback)

rospy.init_node('robot_com', anonymous=True)
rospy.spin()
```

**Services** (Request-Response):
```python
from std_srvs.srv import SetBool, SetBoolResponse

def handle_enable_motor(req):
    # Habilitar/deshabilitar motor
    success = enable_motor(req.data)
    return SetBoolResponse(success=success)

service = rospy.Service('enable_motor', SetBool, handle_enable_motor)
```

**Actions** (Long-running tasks):
```python
import actionlib
from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal

client = actionlib.SimpleActionClient('move_base', MoveBaseAction)
client.wait_for_server()

goal = MoveBaseGoal()
goal.target_pose.header.frame_id = "map"
goal.target_pose.pose.position.x = 1.0
goal.target_pose.pose.position.y = 2.0

client.send_goal(goal)
client.wait_for_result()
```

#### 10.5 Streaming de Video

**GStreamer**:
```bash
# En robot (Raspberry Pi)
rpicam-vid -t 0 --inline -o - | gst-launch-1.0 fdsrc ! h264parse ! rtph264pay config-interval=1 pt=96 ! udpsink host=192.168.1.100 port=5000

# En estaci√≥n base
gst-launch-1.0 udpsrc port=5000 ! application/x-rtp,payload=96 ! rtph264depay ! avdec_h264 ! autovideosink
```

**OpenCV + Socket**:
```python
import cv2
import socket
import pickle
import struct

# Servidor (robot)
cap = cv2.VideoCapture(0)
server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server_socket.bind(('0.0.0.0', 8485))
server_socket.listen(5)

conn, addr = server_socket.accept()

while True:
    ret, frame = cap.read()
    data = pickle.dumps(frame)
    message = struct.pack("Q", len(data)) + data
    conn.sendall(message)
```

#### 10.6 Control Remoto

**Gamepad/Joystick**:
```python
import pygame

pygame.init()
joystick = pygame.joystick.Joystick(0)
joystick.init()

while True:
    pygame.event.pump()
    
    # Leer ejes
    left_x = joystick.get_axis(0)
    left_y = joystick.get_axis(1)
    
    # Leer botones
    button_a = joystick.get_button(0)
    
    # Enviar comandos al robot
    send_velocity_command(left_x, left_y)
```

**App m√≥vil**:
- MIT App Inventor (simple)
- Flutter/React Native (profesional)
- Comunicaci√≥n v√≠a HTTP o WebSocket

#### 10.7 Seguridad de Comunicaci√≥n

**Encriptaci√≥n**:
- TLS/SSL para HTTP (HTTPS)
- Certificados SSL

**Autenticaci√≥n**:
```python
from flask import Flask, request
from functools import wraps

app = Flask(__name__)

def require_api_key(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        api_key = request.headers.get('API-Key')
        if api_key != 'SECRET_KEY_123':
            return jsonify({'error': 'Unauthorized'}), 401
        return f(*args, **kwargs)
    return decorated_function

@app.route('/robot/command', methods=['POST'])
@require_api_key
def command():
    # Comando seguro
    pass
```

**Firewall**:
```bash
# UFW en Raspberry Pi
sudo ufw allow 22/tcp    # SSH
sudo ufw allow 5000/tcp  # API
sudo ufw allow from 192.168.1.0/24  # Red local
sudo ufw enable
```

#### 10.8 Latencia y Quality of Service

**Medici√≥n de latencia**:
```python
import time
import requests

def measure_latency(url, iterations=10):
    latencies = []
    for _ in range(iterations):
        start = time.time()
        requests.get(url)
        latency = (time.time() - start) * 1000  # ms
        latencies.append(latency)
    
    avg_latency = sum(latencies) / len(latencies)
    print(f"Latencia promedio: {avg_latency:.2f}ms")
    return avg_latency
```

**Optimizaci√≥n**:
- Reducir tama√±o de mensajes
- Compresi√≥n (gzip, etc.)
- Priorizaci√≥n de mensajes cr√≠ticos
- Buffering inteligente

### Referencias y Recursos
- üìñ "Computer Networks" - Tanenbaum
- üåê ROS Communication Patterns
- üåê MQTT official documentation

---

## 11. üõ°Ô∏è Seguridad y √âtica

**Carpeta principal**: [`12_Etica_Seguridad/`](../12_Etica_Seguridad/)

### Conocimientos Fundamentales

#### 11.1 Seguridad F√≠sica

**Dise√±o Seguro**:
- **Bordes redondeados**: Evitar cortes
- **Limitaci√≥n de fuerza**: Torque m√°ximo en articulaciones
- **Sensores de colisi√≥n**: Detener ante contacto
- **E-stop (parada de emergencia)**: Bot√≥n f√≠sico accesible

**Ejemplo de limitaci√≥n de fuerza**:
```python
class SafeJoint:
    def __init__(self, max_torque=5.0):  # Newton-metros
        self.max_torque = max_torque
        self.current_sensor = get_current_sensor()
    
    def move_with_safety(self, target_angle):
        torque = calculate_required_torque(target_angle)
        
        if torque > self.max_torque:
            logger.warning("Torque excedido - limitando")
            torque = self.max_torque
        
        # Monitorear corriente (proporcional a torque)
        current = self.current_sensor.read()
        if current > SAFE_CURRENT_LIMIT:
            self.emergency_stop()
            raise SafetyException("Corriente excedida - posible colisi√≥n")
        
        apply_torque(torque)
```

**Sensores de seguridad**:
- Bumpers (parachoques)
- Sensores capacitivos (proximidad humanos)
- Sensores de fuerza en piel artificial
- C√°maras para detecci√≥n de personas

#### 11.2 Zonas de Seguridad

**Clasificaci√≥n de espacios**:
1. **Zona colaborativa**: Robot + humano trabajando juntos
   - Velocidad limitada (<250 mm/s seg√∫n ISO/TS 15066)
   - Detecci√≥n constante de colisi√≥n
   
2. **Zona supervisada**: Humano puede entrar ocasionalmente
   - Robot reduce velocidad cuando detecta persona
   
3. **Zona restringida**: Solo robot
   - M√°xima velocidad
   - Barreras f√≠sicas

#### 11.3 Ciberseguridad

**Amenazas**:
- Acceso no autorizado
- Inyecci√≥n de comandos maliciosos
- Intercepci√≥n de comunicaciones
- DoS (Denial of Service)

**Medidas de protecci√≥n**:

**Autenticaci√≥n**:
```python
import hashlib
import hmac

SECRET_KEY = "super_secret_key"

def verify_command(command, signature):
    """
    Verifica que el comando viene de fuente autorizada
    """
    expected_sig = hmac.new(
        SECRET_KEY.encode(),
        command.encode(),
        hashlib.sha256
    ).hexdigest()
    
    return hmac.compare_digest(signature, expected_sig)

# Uso
if verify_command(received_command, received_signature):
    execute_command(received_command)
else:
    logger.warning("Comando no autorizado rechazado")
```

**Encriptaci√≥n de datos sensibles**:
```python
from cryptography.fernet import Fernet

# Generar clave
key = Fernet.generate_key()
cipher = Fernet(key)

# Encriptar
sensitive_data = "posici√≥n_secreta: x=10, y=20"
encrypted = cipher.encrypt(sensitive_data.encode())

# Desencriptar
decrypted = cipher.decrypt(encrypted).decode()
```

**Actualizaciones seguras**:
- Firma digital de actualizaciones
- Verificaci√≥n de integridad (checksums)
- Rollback autom√°tico si falla

#### 11.4 Privacidad de Datos

**GDPR/Normativas**:
- Consentimiento expl√≠cito para recopilar datos
- Derecho al olvido
- Transparencia en uso de datos
- Minimizaci√≥n de datos

**Anonimizaci√≥n**:
```python
import hashlib

def anonymize_user_id(user_id):
    """
    Anonimiza ID de usuario
    """
    return hashlib.sha256(user_id.encode()).hexdigest()

def collect_telemetry(user_id, data):
    """
    Recolecta datos anonimizados
    """
    anon_id = anonymize_user_id(user_id)
    telemetry = {
        'user': anon_id,
        'timestamp': time.time(),
        'data': data  # Sin informaci√≥n personal
    }
    save_telemetry(telemetry)
```

**Datos sensibles a proteger**:
- Im√°genes/video de caras
- Grabaciones de audio
- Ubicaci√≥n del hogar
- Patrones de comportamiento

#### 11.5 √âtica de la IA

**Principios**:
1. **Transparencia**: Sistema explicable
2. **Equidad**: Sin discriminaci√≥n
3. **Responsabilidad**: Accountability
4. **Privacidad**: Respeto a datos personales
5. **Seguridad**: No causar da√±o

**Detecci√≥n de sesgo**:
```python
def evaluate_fairness(model, test_data, sensitive_attribute):
    """
    Eval√∫a si modelo es justo respecto a atributo sensible
    (ej: g√©nero, edad, etnia - donde sea relevante y legal)
    """
    groups = test_data.groupby(sensitive_attribute)
    
    accuracies = {}
    for group_name, group_data in groups:
        predictions = model.predict(group_data.features)
        accuracy = (predictions == group_data.labels).mean()
        accuracies[group_name] = accuracy
    
    # Disparidad
    max_acc = max(accuracies.values())
    min_acc = min(accuracies.values())
    disparity = max_acc - min_acc
    
    if disparity > 0.1:  # 10% diferencia
        logger.warning(f"Posible sesgo detectado: {accuracies}")
    
    return accuracies
```

**Explicabilidad** (XAI):
```python
import shap

# SHAP values para explicar predicciones
explainer = shap.Explainer(model)
shap_values = explainer(X_test)

# Visualizar qu√© caracter√≠sticas influyeron
shap.plots.waterfall(shap_values[0])
```

#### 11.6 Leyes de la Rob√≥tica (Asimov - filos√≥ficas)

1. Un robot no puede da√±ar a un humano
2. Un robot debe obedecer √≥rdenes (excepto si viola #1)
3. Un robot debe proteger su existencia (excepto si viola #1 o #2)

**Implementaci√≥n conceptual**:
```python
class EthicalDecisionMaker:
    def evaluate_action(self, action):
        # Ley 1: ¬øDa√±a a humano?
        if self.will_harm_human(action):
            return False, "Viola Ley 1: Posible da√±o a humano"
        
        # Ley 2: ¬øEs orden de humano?
        if action.is_human_order:
            return True, "Cumple Ley 2: Orden humana"
        
        # Ley 3: ¬øEs auto-preservaci√≥n?
        if action.is_self_preservation:
            return True, "Cumple Ley 3: Auto-preservaci√≥n"
        
        return True, "Acci√≥n neutral"
    
    def will_harm_human(self, action):
        # Simular acci√≥n y predecir resultado
        simulation = self.physics_engine.simulate(action)
        
        # Detectar humanos en √°rea
        humans_detected = self.perception.detect_humans()
        
        for human in humans_detected:
            if simulation.collision_with(human):
                return True
            if simulation.force_on(human) > SAFE_FORCE_THRESHOLD:
                return True
        
        return False
```

#### 11.7 Normativas y Est√°ndares

**ISO/TS 15066**: Robots colaborativos
- L√≠mites de fuerza y presi√≥n
- Requisitos de seguridad

**ISO 13482**: Robots de cuidado personal
- Seguridad en interacci√≥n humano-robot

**ISO 10218**: Robots industriales
- Requisitos generales de seguridad

**IEC 60950/62368**: Seguridad el√©ctrica
- Protecci√≥n contra choques el√©ctricos
- Aislamiento

#### 11.8 Gesti√≥n de Riesgos

**An√°lisis FMEA** (Failure Mode and Effects Analysis):
```
| Modo de Fallo | Efecto | Severidad | Probabilidad | Detecci√≥n | RPN | Mitigaci√≥n |
|---------------|--------|-----------|--------------|-----------|-----|------------|
| Motor falla | Ca√≠da | 8 | 3 | 7 | 168 | Redundancia, e-stop |
| Software crash | Movimiento err√°tico | 9 | 4 | 5 | 180 | Watchdog, failsafe |
| Bater√≠a agotada | Parada s√∫bita | 6 | 5 | 2 | 60 | Alerta temprana |
```

RPN = Severidad √ó Probabilidad √ó Detecci√≥n (priorizar si >100)

**Plan de contingencia**:
```python
class EmergencyHandler:
    def __init__(self):
        self.watchdog = Watchdog(timeout=2.0)
        self.safe_state = SafeState()
    
    def monitor(self):
        try:
            # Bucle principal
            while True:
                self.watchdog.kick()
                robot.update()
                
                # Verificar condiciones
                if battery.voltage < CRITICAL_VOLTAGE:
                    self.handle_low_battery()
                
                if not self.watchdog.is_alive():
                    self.emergency_stop()
                    
        except Exception as e:
            logger.critical(f"Exception cr√≠tica: {e}")
            self.emergency_stop()
    
    def emergency_stop(self):
        logger.critical("EMERGENCY STOP")
        # Detener todos los motores
        for motor in robot.motors:
            motor.stop()
        # Posici√≥n segura (ej: agacharse lentamente)
        robot.move_to_safe_position()
        # Notificar
        send_alert("Robot en modo seguro")
```

### Referencias y Recursos
- üìñ "Robot Ethics" - Lin, Abney, Bekey
- üìñ "AI Ethics" - Coeckelbergh
- üåê ISO standards (ISO.org)
- üåê EU AI Act
- üåê IEEE Ethically Aligned Design

---

## 12. üìä Gesti√≥n de Proyectos

**Carpeta principal**: [`00_Gestion_Proyecto/`](../00_Gestion_Proyecto/)

### Conocimientos Fundamentales

#### 12.1 Metodolog√≠as

**Cascada (Waterfall)**:
```
Requisitos ‚Üí Dise√±o ‚Üí Implementaci√≥n ‚Üí Pruebas ‚Üí Despliegue
```
- Secuencial
- Bueno para requisitos bien definidos

**√Ågil (Agile)**:
- Iterativo e incremental
- Sprints de 1-4 semanas
- Adaptable a cambios

**Scrum**:
- Roles: Product Owner, Scrum Master, Equipo
- Eventos: Sprint Planning, Daily Standup, Review, Retrospective
- Artefactos: Product Backlog, Sprint Backlog, Increment

**Adaptaci√≥n para proyecto personal**:
- Sprints semanales
- Revisi√≥n semanal (retrospectiva)
- Backlog priorizado

#### 12.2 Planificaci√≥n

**WBS** (Work Breakdown Structure):
```
Robot Humanoide
‚îú‚îÄ‚îÄ 1. Dise√±o
‚îÇ   ‚îú‚îÄ‚îÄ 1.1 Mec√°nico
‚îÇ   ‚îú‚îÄ‚îÄ 1.2 Electr√≥nico
‚îÇ   ‚îî‚îÄ‚îÄ 1.3 Software
‚îú‚îÄ‚îÄ 2. Fabricaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ 2.1 Impresi√≥n 3D
‚îÇ   ‚îú‚îÄ‚îÄ 2.2 Ensamblaje
‚îÇ   ‚îî‚îÄ‚îÄ 2.3 Soldadura
‚îú‚îÄ‚îÄ 3. Programaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ 3.1 Control bajo nivel
‚îÇ   ‚îú‚îÄ‚îÄ 3.2 Percepci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ 3.3 IA
‚îî‚îÄ‚îÄ 4. Pruebas
    ‚îú‚îÄ‚îÄ 4.1 Unitarias
    ‚îú‚îÄ‚îÄ 4.2 Integraci√≥n
    ‚îî‚îÄ‚îÄ 4.3 Sistema
```

**Diagrama de Gantt**:
```
Tarea                | Mes 1 | Mes 2 | Mes 3 | Mes 4 |
---------------------|-------|-------|-------|-------|
Dise√±o Mec√°nico      |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|       |       |       |
Dise√±o Electr√≥nico   |  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|     |       |       |
Fabricaci√≥n          |       |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|       |       |
Programaci√≥n         |       |   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|       |
Integraci√≥n          |       |       |   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|     |
Pruebas              |       |       |       |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|
```

**Ruta cr√≠tica**:
- Secuencia de tareas que determina duraci√≥n m√≠nima
- Identificar para priorizar recursos

#### 12.3 Gesti√≥n de Recursos

**Presupuesto ejemplo**:
```
Categor√≠a          | Cantidad | Costo Unit | Total
-------------------|----------|------------|-------
Mec√°nica           |          |            |
- Aluminio         | 5 kg     | $10/kg     | $50
- Filamento PLA    | 5 kg     | $20/kg     | $100
- Torniller√≠a      | Lote     | $50        | $50
                   |          |            |
Electr√≥nica        |          |            |
- Raspberry Pi 4   | 1        | $75        | $75
- Arduino Mega     | 2        | $40        | $80
- Servomotores     | 20       | $15        | $300
- Sensores (IMU, cam) | Varios | $150    | $150
- Bater√≠as LiPo    | 2        | $60        | $120
                   |          |            |
Software           |          |            |
- Licencias (si aplica) | -   | $0         | $0
                   |          |            |
Herramientas       |          |            |
- Mult√≠metro       | 1        | $30        | $30
- Soldador         | 1        | $40        | $40
                   |          |            |
Contingencia (20%) |          |            | $199
                   |          |            |
**TOTAL**          |          |            | **$1,194**
```

#### 12.4 Gesti√≥n de Riesgos

**Registro de riesgos**:
```
| ID | Riesgo | Probabilidad | Impacto | Mitigaci√≥n | Contingencia |
|----|--------|--------------|---------|------------|--------------|
| R1 | Pieza no encaja | Media | Alto | Tolerancias en dise√±o | Redise√±ar |
| R2 | Sensor defectuoso | Baja | Medio | Comprar repuestos | Reemplazar |
| R3 | Retraso entregas | Alta | Medio | Pedir con anticipaci√≥n | Proveedores alternativos |
| R4 | Bug cr√≠tico | Media | Alto | Testing exhaustivo | Rollback |
```

#### 12.5 Seguimiento y Control

**KPIs** (Key Performance Indicators):
- **Cumplimiento cronograma**: % tareas a tiempo
- **Presupuesto**: Gasto vs. planificado
- **Calidad**: # bugs, # pruebas pasadas
- **Velocidad**: Tareas completadas por sprint

**Herramientas**:
- Trello/Asana: Gesti√≥n de tareas
- GitHub Projects: Integrado con c√≥digo
- Notion: Documentaci√≥n y planificaci√≥n
- Excel/Sheets: Tracking manual

#### 12.6 Documentaci√≥n Continua

**Estructura**:
```
docs/
‚îú‚îÄ‚îÄ architecture/
‚îÇ   ‚îú‚îÄ‚îÄ hardware_design.md
‚îÇ   ‚îî‚îÄ‚îÄ software_architecture.md
‚îú‚îÄ‚îÄ user_manual/
‚îÇ   ‚îú‚îÄ‚îÄ setup.md
‚îÇ   ‚îú‚îÄ‚îÄ operation.md
‚îÇ   ‚îî‚îÄ‚îÄ troubleshooting.md
‚îú‚îÄ‚îÄ developer/
‚îÇ   ‚îú‚îÄ‚îÄ api_reference.md
‚îÇ   ‚îú‚îÄ‚îÄ contributing.md
‚îÇ   ‚îî‚îÄ‚îÄ testing.md
‚îú‚îÄ‚îÄ decisions/
‚îÇ   ‚îî‚îÄ‚îÄ ADR-001-choice-of-microcontroller.md
‚îî‚îÄ‚îÄ meetings/
    ‚îî‚îÄ‚îÄ 2026-02-27-weekly-review.md
```

**ADR** (Architecture Decision Records):
```markdown
# ADR-001: Elecci√≥n de ROS como Framework

## Contexto
Necesitamos un framework para integrar sensores, actuadores y algoritmos.

## Decisi√≥n
Usar ROS (Robot Operating System) versi√≥n Noetic.

## Consecuencias
**Positivo**:
- Ecosistema amplio
- Herramientas de visualizaci√≥n (RViz)
- Comunidad activa

**Negativo**:
- Curva de aprendizaje
- Overhead en sistemas simples

## Alternativas consideradas
- Framework propio (demasiado esfuerzo)
- Middleware ligero (menos funcionalidad)
```

### Referencias y Recursos
- üìñ "The Lean Startup" - Eric Ries
- üìñ "Scrum: The Art of Doing Twice the Work in Half the Time" - Jeff Sutherland
- üåê PM BoK (Project Management Body of Knowledge)

---

## üìå Resumen por Carpeta del Proyecto

| Carpeta | Conocimientos Principales |
|---------|---------------------------|
| [`00_Gestion_Proyecto/`](../00_Gestion_Proyecto/) | **12** Gesti√≥n de Proyectos |
| [`01_Percepcion/`](../01_Percepcion/) | Sensores, fusi√≥n sensorial, filtrado |
| [`02_Localizacion_Mapeo/`](../02_Localizacion_Mapeo/) | SLAM, odometr√≠a, mapeo |
| [`03_Planificacion/`](../03_Planificacion/) | **3** Programaci√≥n, planificaci√≥n de trayectorias |
| [`04_Control/`](../04_Control/) | **3** Programaci√≥n y Control, **9** Biomec√°nica |
| [`05_Aprendizaje_Maquina/`](../05_Aprendizaje_Maquina/) | **6** IA y Aprendizaje Autom√°tico |
| [`06_Vision/`](../06_Vision/) | **6** Visi√≥n por Computadora, detecci√≥n objetos |
| [`07_Simulacion_Pruebas/`](../07_Simulacion_Pruebas/) | Gazebo, PyBullet, testing |
| [`08_Integracion_Hardware/`](../08_Integracion_Hardware/) | **1** Mec√°nica, **2** Electr√≥nica, **4** Mecatr√≥nica, **7** Materiales, **8** Energ√≠a |
| [`09_Comunicaciones_Interfaces/`](../09_Comunicaciones_Interfaces/) | **5** Dise√±o de Software, **10** Comunicaci√≥n |
| [`10_Datasets_Experimentos/`](../10_Datasets_Experimentos/) | Datos de entrenamiento, experimentos |
| [`11_Herramientas_Utilidades/`](../11_Herramientas_Utilidades/) | Scripts, utilidades |
| [`12_Etica_Seguridad/`](../12_Etica_Seguridad/) | **11** Seguridad y √âtica |

---

## üéì Recomendaciones de Aprendizaje

### Ruta de Aprendizaje

**Fase 1: Fundamentos (Meses 1-3)**
1. Python (b√°sico ‚Üí avanzado)
2. Electr√≥nica b√°sica
3. Mec√°nica b√°sica
4. Linux/Bash

**Fase 2: Rob√≥tica (Meses 4-6)**
1. ROS
2. Cinem√°tica y control
3. Visi√≥n por computadora
4. Dise√±o CAD

**Fase 3: IA y Avanzado (Meses 7-12)**
1. Machine Learning
2. Deep Learning
3. Aprendizaje por refuerzo
4. Integraci√≥n completa

### Recursos Online Gratuitos

**Cursos**:
- Coursera: "Robotics Specialization" (UPenn)
- edX: "Autonomous Mobile Robots" (ETH Zurich)
- YouTube: "MIT OpenCourseWare - Robotics"
- YouTube: "Sentdex - ROS tutorials"

**Documentaci√≥n**:
- ROS Wiki: wiki.ros.org
- PyTorch Tutorials: pytorch.org/tutorials
- OpenCV Docs: docs.opencv.org

**Comunidades**:
- r/robotics (Reddit)
- ROS Discourse
- Robotics Stack Exchange
- DIY Drones Forum

---

**√öltima actualizaci√≥n**: Febrero 2026

**Nota**: Este es un documento vivo - actualizar conforme se adquieren nuevos conocimientos y se descubren mejores pr√°cticas.
